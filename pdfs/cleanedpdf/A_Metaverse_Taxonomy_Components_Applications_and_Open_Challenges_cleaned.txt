Received November , , accepted December , , date of publication January , , date of current version January , . Digital Object Identifier A Metaverse: Taxonomy, Components, Applications, and Open Challenges SANG-MIN PARK  AND YOUNG-GAB KIM , (Member, IEEE) 1Department of Computer Science and Engineering, Korea University, Seoul , South Korea 2Department of Computer and Information Security, and Convergence Engineering for Intelligent Drone, Sejong University, Seoul , South Korea Corresponding author: Young-Gab Kim (alwaysgabi@sejong.ac.kr) This work was supported by the National Research Foundation of Korea (NRF) funded by the Korean Government (MSIT) under ABSTRACT Unlike previous studies on the Metaverse based on Second Life, the current Metaverse is based on the social value of Generation Z that online and ofﬂine selves are not different. With the technological development of deep learning-based high-precision recognition models and natural generation models, Metaverse is being strengthened with various factors, from mobile-based always-on access to connectivity with reality using virtual currency. The integration of enhanced social activities and neural- net methods requires a new deﬁnition of Metaverse suitable for the present, different from the previous Metaverse. This paper divides the concepts and essential techniques necessary for realizing the Metaverse into three components (i.e., hardware, software, and contents) and three approaches (i.e., user interaction, implementation, and application) rather than marketing or hardware approach to conduct a comprehensive analysis. Furthermore, we describe essential methods based on three components and techniques to Metaverse’s representative Ready Player One, Roblox, and Facebook research in the domain of ﬁlms, games, and studies. Finally, we summarize the limitations and directions for implementing the immersive Metaverse as social inﬂuences, constraints, and open challenges. INDEX TERMS Artiﬁcial intelligence, metaverse, cyber world, avatar, extended reality. Metaverse is expanding rapidly, as seen in Geppetto serving  million subscribers and Animal Crossing running an election campaign in a virtual space. In particular, Roblox’s monthly active users (MAU) is  million, which is used by / of children aged - in the US, and / of them are under  []–[]. Early studies for the Metaverse focus on Second Life in  []–[]. However, the current Metaverse is based on the social values of Generation Z that online ego is no different from ofﬂine ones []. Therefore, since the proportion of social activities and contents grows, it differs from the previous Metaverse, and a new deﬁnition is needed The novel Metaverse differs from the earlier Metaverse learning dramatically improves the accuracy of vision and language recognition, and the development of generative The associate editor coordinating the review of this manuscript and approving it for publication was Sudhakar Babu Thanikanti models enables a more immersive environment and natural movement. The processing time and complexity were reduced using multimodal models as E2E (end-to-end) solutions with a multimodal pre-trained model. Second, Metaverse previously served based on PC access and had low consistency due to time and space constraints, but now it is possible to easily access the Metaverse anytime, anywhere due to the mobile devices that can connect to the Internet at all times. There are  million games in Roblox and the accumulated monthly usage time is  billion hours. People consums more time than social network services (e.g., TikTok, YouTube). It has a virtuous cycle ecosystem in which the inﬂow and income of producers increase as users and usage time increase while serving various contents, and thus sales of digital advertisements increase. Lastly, the current Metaverse differs from the previous one because the program coding can be done in the Metaverse world, and it is more bonded to real life with virtual currency. Metaverse expands with various social meanings (e.g., fashion, event, game, education, and ofﬁce) based on immersive interaction. This work is licensed under a Creative Commons Attribution . License. For more information, see https://creativecommons.org/licenses/by/./ S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges Cryptocurrencies (e.g., Dime) serve as an economic bridge between the Metaverse and the real world, giving people The Metaverse differs from augmented reality (AR) and virtual reality (VR) in three ways. First, while VR-related studies focus on a physical approach and rendering, Meta- verse has a strong aspect as a service with more sustainable content and social meaning. Second, the Metaverse does not necessarily use AR and VR technologies. Even if the platform does not support VR and AR, it can be a Metaverse application. Lastly, the Metaverse has a scalable environment that can accommodate many people is essential to reinforce social meaning. The large-scale Metaverse implementation required three components: (i) hardware improvements (e.g., GPU memory, 5G); (ii) the development of the recognition and expression model that leverages the parallelism of the hardware; and (iii) the availability of content that people Despite the considerable research relating to Metaverse, primarily focus on social meaning, and little attention has focused on technologies for the Metaverse. For example, a systematic approach to what concepts and technologies are required to create an environment and content users can enjoy in Ready Player One is needed. Beyond simply creating a physical, virtual space, provide an immersive experience with a story through user interaction. This research presents a comprehensive study on the applications and technologies that can give social meaning in a Metaverse hardware, software, and content with three approaches (i.e., user interactions, implementation, and Firstly, this study analyzed hardware components, software components, and contents into component levels to create an immersive experience in the Metaverse, as shown in Fig. . In order to give the user a sense of visual immersion, a lightweight head-mounted display (HMD) and physical auxiliary devices are required to use for a long time with high-resolution images []. In terms of software, as the delay increases, dizziness and motion sickness occur due to sensory confusion, so low delay and fast rendering are important. In addition, since Metaverse is conducted based on a wide -degree ﬁeld of view, large-capacity vision data processing and generative recognition of obscured objects are signiﬁcant issues. There is a technical gap in hardware and software performance compared to users’ expectations for the Metaverse. The natural movement of the graphical envi- ronment displayed in the Metaverse can give an immersive feeling. However, to provide a sustainable service, it is crucial to have immersive contents that work even with limited hardware and low-resolution software (e.g., Minecraft). For sustainable Metaverse content, there must be a plot that con- siders various user interactions in the virtual environments. In other words, an approach in the form of a complete story (e.g., a movie and a drama) is needed rather than several dialogues turn. Because user-created content is not produced with a large number of organized teams, methods (e.g., persona generation, cartoon generation) to complement professionalism are needed. In addition, multimodal-based stories based on immersive interaction can be used effectively in the Metaverse to implement such an interactive user Secondly, this study analyzed user interactions, imple- mentations, and applications into approach levels to provide a stable experience in the Metaverse, as shown in Fig. . Comprehensive recognition and interpretation through mul- timodal inference are required for an interaction that can effectively utilize the technologically growing hardware and software performance. For example, human-robot interaction and visual-language interaction are similar in that they are egocentric views to be used as element technologies for user interactions. Metaverse environments are divided into service platforms (e.g., Roblox, Minecraft) and conﬁgurable environments (e.g., Unity) for implementations. In order for the Metaverse to allow many people to live life in the same space, infrastructure elements (e.g., wide bandwidth network connection, fault management, and security) are also important for implementations. The details of each appli- cation and event also play an important role in composing the Metaverse. As applications and events (e.g., simulation, marketing, and education) become more concrete, people’s activities will increase, and their playing time gradually increases accordingly [], []. Due to the wide scope of the Metaverse, we lack a clear understanding of how they work, why they need, and what they are even capable of due to their novel component. To tackle these problems deeply, we require interdisciplinary collaboration and research with the psychology and social This study has three main contributions as follows. • Metaverse taxonomy is proposed by summarized tech- nologies and is used to classify the studies of research institutes. We classiﬁed the Metaverse components and major approaches into hardware, software, contents, user implementations, and applications. For each approach, we have summarized the technologies that have recently become issues and interests. • We classify Metaverse’s representative Ready Player One, Roblox, and Facebook research in ﬁlms, games, and studies using the method deﬁned above and describe the latest • Finally, problems and directions in implementing an immersive Metaverse are divided into social restrictions, and open challenges. The remainder of this study is organized as follows. In Section , various deﬁnitions of Metaverse and avatar are arranged in chronological order. Section  describes and pro- poses three components necessary, and Section  describes the high-level approach to give an immersive experience In Section , we verify how the deﬁned components and approaches are used through case studies on Ready player one, Roblox, and Facebook limitations, and open challenges S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges are discussed in Section  and ﬁnally concluded in This section describes the concepts of the Metaverse, avatar, and extended reality (XR) based on differences of similar concepts. The Metaverse refers to the virtual world in which the avatar acts, and the avatar is the user’s alter ego and becomes the active subject in the Metaverse. XR is the medium that connects avatars in Metaverse and users in the In this paper, we partially utilized systematic literature reviews (SLRs) techniques to obtain reliable references []. The reclusive procedure for selecting references is: ) search by combining related keywords ) extract papers that contain keywords in the title and body ) remove papers that contain keywords but are not directly related to the Metaverse ) cluster related papers ) conﬁgure taxonomy. At ﬁrst, we extract keywords (i.e., Metaverse, Avatar, Extended Reality) for Metaverse concepts that are interpreted in various forms, as shown in Table . As depicted in Fig. , each paper’s Metaverse deﬁnitions and characteristics are analyzed in chronological order out of a total of  papers, including  papers of Elsevier papers and  papers of Google scholar based on relevance. Table  summarizes the deﬁnitions and main viewpoints of  papers that speciﬁcally describe the Metaverse [], []–[], []–[]. In Section III, we make component taxonomy (i.e., hard- ware, software, contents) which is necessary to construct the Metaverse with the same procedure including a total of  sub-categories. In a similar way, we construct an approach inclduing  sub-categories for Metaverse approaches in Section IV. Finally, we choose representative services of the Metaverse and evaluate taxonomy by mapping references. Especially in the case of Facebook, we review papers that are announced on the papers published in Facebook Research Duan et al. [] presented the representative applications in the aspect of infrastructure, interaction, and ecosystem. They also provide a three-layer Metaverse architecture containing ana brief timeline for Metaverse development. Messinger et al. [] introduced a virtual world where thousands of people can interact simultaneously within the same simulated 3D space. It covered the perspectives of business, education, social sciences, technical sciences, and social computing that affect our society as a whole. Müller [] deﬁned the world as an electronic memory and the Internet as a virtual reality where users log in every day. They focus on safely preserving information, the evaluation of data, and perception. Dionisio et al. [] focused on the ubiquity of access and identity, interoperability, and scalability of Metaverse. Nevelsteen [] focused on ontology as the relation of the complimentary terms and acronyms. They also introduced the usage of pseudo persistence to categories technologies that only mimic persistence. Since various Metaverse surveys mainly focus on application and social meaning, comprehensive research on Metaverse technology is lacking. In order to compose the Metaverse, it is necessary to investigate a comprehensive view of the latest technology components, approaches, and services. We compare Metaverses deﬁned in  other surveys in Table  and dealt with HW, SW, and Contents in depth. In particular, we evaluate the proposed taxonomy through three different types of use cases. Metaverse is a compound word of transcendence meta and universe and refers to a three-dimensional virtual world where avatars engage in political, economic, social, and cultural activities. It is widely used in the sense of a virtual world based on daily life where both the real and the unreal coexist []. Metaverse was ﬁrst used in Neil Stevenson’s science ﬁction novel Snow Crash in  and referred to a world where virtual and reality interact and create value through various social activities []. As the scope of the Metaverse is wide and continuously growing, various deﬁnitions and similar concepts exist. Lee et al. [] divided life-logging, mirror world, augmented reality, and the virtual world according to whether the implemented space is reality- oriented or virtual-centered, and whether the implemented information is external environment information-centered focused on the composition of the virtual world itself (e.g., for exchanging interests and social interaction centered on Mirror world (e.g., Google Earth, Microsoft Virtual Earth) refers to extending information into the virtual world by realistically reﬂecting the real world. Mirror World is originated from a book called Mirror Worlds written by David Gelernter in  []. The real space where people live is reproduced in digital form, and additional simulation information is added. In other words, the mirror world replicates the appearance of buildings or objects in the real world but has its own properties and functions. Metaverse, multiverse, digital terraforming, and mirror world are conceptually similar but have slightly different meanings depending on where they are used and share some concepts. An avatar means an alter ego that has descended to the earth, and it started from the concept that a fundamental being (e.g., God) changed its form to human. Previously, the avatar was used as a pre-deﬁned exaggerated form in the virtual world rather than reﬂecting the real world. However, it gradually changes into an ideal form that projects the outward appearance and reﬂects the ego. An avatar performs a social role suitable for a job and persona in Metaverse. In particular, costumes and items in Metaverse are used as a S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges FIGURE . Organization of the paper. medium to express the social meaning of avatars, and various luxury clothing companies are paying attention and selling them. The younger generation considers the social meaning of the virtual world as important as the real world, as they think that their identity in virtual space and reality is the S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges FIGURE . Metaverse papers over the time sequence. Avatar, the subject of the Metaverse, has a similar meaning to the digital twin and digital Me of the virtual world. A digital twin is a virtual model for predicting behavior []. Digital twins are used to create real-object-like agents in the virtual world and predict outcomes in advance through simulations of situations that might occur in real life. Initially proposed information representing contexts and processes of various physical entities to understand past and present operating states. It is used to maintain properties and states throughout the lifecycle of a digital twin and predict what will happen in the future. It can optimize the physical world and is used in various industrial and social issues and manufacturing to improve operational performance and business processes signiﬁcantly. Digital Me is a symbolic expression of ego in a digital world that is different from the actual self. Conceptually, the digital twin is different in that it objectively interprets the real self, whereas digital Me interprets it subjectively. In terms of application, digital twins are used to solve current problems and simulate future outcomes. Digital Me, on the other hand, is a surrogate self that projects one’s self that cannot be done in real life. In terms of technology, XR is related to VR (virtual reality), AR (augmented reality), and MR (mixed reality). VR used to act as an avatar in a digitally implemented three-dimensional world (e.g., ZEPETO). VR provides an S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges experience as if you were in a speciﬁc place without physical limitations, helping you learn about the ideas you can get from experiencing different places. While VR is a technology that allows a new reality to compete based on -degree S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges TABLE . (Continued.) Metaverse definition. images, AR is a method of superimposing virtual objects on real space from a ﬁrst-person perspective (e.g., Pokemon Go). AR overlays computer-generated images, sounds, 3D models, videos, graphics, animated sequences, games, and S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges TABLE . (Continued.) Metaverse definition. GPS information into real-world environments [], []. Visually search for objects and adjust interfaces by overlaying visually immersive content in the real world. In particular, it has the advantage of clearly providing information and visualizing controllable devices without an additional MR, which integrates these two concepts, is a mixed reality technology that integrates VR and AR. MR is the concept of creating virtual objects that allow users to interact with the 3D environment in the immersion of the virtual environment of VR and the overlay of virtual content in AR. AR provides a more realistic solution because the hardware is relatively simple, like glasses, and reﬂects reality well, but it is suitable for short content []. On the other hand, VR covers the entire ﬁeld of view, has an immersive feeling, and is suitable for long-term content but entails physical fatigue. In some cases, MR, which uses a mixture of these advantages and disadvantages, is being considered as a solution that can be converted to AR and VR with a single device. XR is an extended reality, which is terms used to include VR, AR, and MR. XR is used for virtual commerce or v-commerce to create computer-mediated indirect experiences []. Metaverse gives patients an immersive experience enough to be used in psychotherapy. People know that myths and Metaverse is not the real world but can provide a tangible feeling, so services based on immersive user-interactive stories can provide. A representative example of such an approach is a game based on two-way interaction. In order to service the Metaverse like the real world, it is necessary to be able to interact seamlessly and concurrency in an environment with presence. In order to maintain a sustainable Metaverse, economic activity between users based on these interactions must continue. We describe Metaverse into hardware, software, and contents from the component’s point A. HARDWARE COMPONENTS (PHYSICAL DEVICES AND Hardware in Metaverse not only plays an important role in the immersive experience but also is a technically limiting barrier. In the Metaverse, hardware is quickly enhanced by the effects of technological advancement, but it still needs improvement compared to the experience of the real world. The essential hardware of Metaverse is an HMD that blocks the view to enable immersive participation. For a more effective visual experience, Birnie et al. [] proposed a fovea rendering method that maintains the central part in high resolution similar to human vision. Critical factors for physical devices and sensors are resolution, the ﬁeld of view size, and latency. Among them, most important characteristic is latency, which plays an important role in multimodal interactions, so it should be designed considering the threshold for side effects and The HMD shows an image through the display and plays the role of playing the sound through the speaker []. HMD is a basic input tool of Metaverse and is divided into Non- see-through HMD, Optical-see-through HMD, and video- see-though HMD []. In the case of a method that covers the screen, it provides a sense of immersion in a completely virtual world. Optical-see-through (mainly used in AR) is a method of overlaying the virtual world, and high hardware speciﬁcations are required in the process of overlaying. To complement this method, video-see-though HMD is used. These HMD issues are the bulky, expensive, and short battery life of the headset. HMD tracks position and orientation according to the movement of the head and delivers the same change of view as in the virtual world by moving the screen. It is more inaccurate than the method of estimating motion by external measurement due to problems with accuracy and delay time, but it is widely used because it can save space and S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges FIGURE . The example of circular coordination and area for hand-based input device []. Diverse circular coordination and input area are proposed for hand-based input devices as shown in Fig.  []. Detailed user data modeling (e.g., mobile phone grip prediction) is required to provide feeling the material with tactile. Haptic has a passive haptic that gives the texture of real objects and an active haptic that creates virtual pressure. Passive haptic is used to help understand the situation while giving presence, and active haptic is used for more effective interaction by adjusting and delivering according to user feedback. Using real props (e.g., physical degree and operational degree) in a virtual environment helps the user experience, while using a robotized interface allows for more diverse interactions []. Depending on the device’s installation, it is divided into the case of being attached to the hand and the case of being attached to the outside. Beyond making the material is used in various forms (e.g., tracking, voice input device, and so on []. Eye-tracking is a method of changing the viewpoint by predicting eye movement when the user moves their eyes without turning their heads. It is a technology that allows the user to see what kind of object the user is paying attention to. It has the advantage of reducing the load on image processing by generating high-resolution images in the section where the user is focused on a phobia method. The method of overlaying the display on the arm is more stable than the method in the air by repeatedly providing the display at a location predictable by the user []. Voice input has an advantage in processing long texts and conversations in a virtual keyboard and an environment where input is In order to effectively use the physical sense of space or gravity, body tracking and treadmill are used to provide accurate motion information with auxiliary devices. Motion input devices are also divided into a passive method and an active method. The passive method is a method of delivering a sense to the user with a ﬁxed scenario, and the active method is a method of providing appropriate feedback based on the user’s behavior. It is used in various forms to give realism, from a simple way to walking to a -degree rotation. There is a risk of injury to the user, so a method of ﬁxing the waist B. SOFTWARE COMPONENTS (RECOGNITION AND A cognitive illusion plays an essential role in immersion in the objective reality of the physical space and the subjective reality that users feel. There are two types of cognition: static cognition and dynamic cognition. Static cognition is the proprioceptive senses (e.g., sight, hearing, and touch), while dynamic cognition is sensory balance and body movement []. In dynamic cognition, adaptation, attention, and behavior are important features. According to the object of cognition, it can be divided into the cognition of environment and cognition of an object. In particular, in Metaverse, it is important to reduce the distortion of detection and recognition. Methods for mitigating distortion include changing the shape of the kernel, changing the expression, and increasing the input. Objects of object recognition include faces, poses, gestures, and gazes related to the body. Such object recognition goes through the process of sensing, recording, recognizing, and tracking. There are two types of stimulation: remote and proximity stimulation. There are bottom-up and top-down approaches to perceiving stimuli. A concept of perception that is distinct from this intuitive sense is also needed. The unconscious S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges FIGURE . Scene rendering for visual language navigation with three-dimension []. approach and the conscious approach are classiﬁed according to the presence or absence of a difference in movement according to repetitive recognition. There are instinc- tive, behavioral, contemplative, and emotional processing The avatar is an important entity in the Metaverse, and the avatar is created, and the action is imitated using animation. Vision-based models estimate human poses, recognize hand gestures and predict gaze. To predict the gaze, iris, facial contour, and 3D gaze prediction are used. ) SCENE AND OBJECT RECOGNITION Object recognition is the process of recognizing the size, shape, position, brightness, and colors of objects according to distance. For scene recognition and object recognition, novel methods (e.g., modal alignment, cross-modal attention, point cloud, and scene graph) are used as shown in Fig.  []. Scene recognition is a good recognition of what state the current scene is and what components and conﬁgurations it has. In sub-graph-based scene graph generation, a method of clustering object pairs into graphs by clustering and sharing representations is used []. Scene graphs are a good approach to complement the explainable properties that have emerged as limitations of neural network models. Some studies use generative methods and scene graphs to classify bodies in overlapping situations and predict human postures Object recognition is also important along with scene recognition, and we have to pay attention to human- centered scene analysis and non-contact interaction (e.g., gaze, gesture, pose). When many objects are recognized using individual object detection, the number of computations increases in proportion to the number of objects, so an attempt is made to reduce the computational burden by using an abstraction concept. In particular, some studies (e.g., world models and MONET) abstract multiple objects into representations for fast object recognition and efﬁcient ) SOUND AND SPEECH RECOGNITION Recognizing sounds and processing speech help understand surroundings and communicate with other avatars. The conversation is a direct method of communication with other avatars and giving instructions to NPCs in Metaverse. As the Metaverse connection is made in various environments, is necessary to have a technology that separates the surrounding noise and one’s own voice without noise. In addition, the loudness of the sound according to the distance is a variable. For a realistic environment in the Metaverse, voice recognition technology is needed that considers the surrounding environment while adjusting the volume according to the distance. The method of generating the environment and objects in Metaverse is divided into the method of depicting by reﬂecting the real world and the method of creating a new imaginary environment. A realistic way to reﬂect the real- is to reproduce famous places (e.g., museums, Eiffel Tower) and places familiar to individuals (e.g., home, school) in the real world. Alternatively, it creates a hard-to-reach environment (e.g., underwater, Mars) to provide a surreal experience. People and things are the main objects of object generation. Object generation modules create an avatar and NPC of any desired human shape (e.g., a celebrity, a family member) as an object of conversation. It focuses on facial expressions and natural movements of joints for ﬂuent multimodal conversation. On the other hand, it generates realistic objects that express in detail enough to feel the texture of objects that exist in reality. On the other hand, another type of object is imaginary animals (e.g., unicorns, dragons) and anthropomorphic objects (e.g., talking Sound synthesis is a ﬁeld that gives the user a sense of immersion, but research is insufﬁcient compared to vision. It creates a sound in the space to give a feeling of presence in the ﬁeld and to increase the sense of immersion. In particular, a voice suitable for each character is an important means of expressing the character’s persona. Tacotron, a speech synthesis, focuses on that users can use prosody to emphasize words or express uncertainty []. Prosody is the variation of the speech signal that remains after taking the variation into account (e.g., phonetics and channel effects), which captures meaningful utterances and transfers them by subtractive CNNs and global context encoding are used to capture asymmetric dependencies and context patterns between objects in real-time multi-party 3D motion capture and S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges pose estimation [], []. The graph reﬂects the structural characteristics of the body to interpret the action meaning more accurately when the human body is superimposed. Although it is possible to capture the real-time 3D motion of difﬁcult scenes with a single-color camera and isolate human body structures (e.g., shaking hands), it is still limited in capturing close interactions (e.g., hugs). C. CONTENTS (SCENARIO AND STORY) Content is the fundamental component that maintains the Metaverse and is used to provide an immersive experience through well-organized stories and user-created events. In content, story reality, immersive experience, and con- ceptual completeness are important. There are two ways to create content, a paradigm shift method and a method to reuse existing content. The areas that require environment design are scenes, color and lighting, audio, sampling and aliasing, environmental navigation, and real-world content. User motions, characters, and the persona of avatars affect Wang et al. [] introduced studies to process panoramic images and videos in virtual 3D scenes using CNNs and GANs to generate and explore VR content. The generated sentences and images become more natural than before, but sentence patterns with similar meanings are sometimes repeated and evaluated superﬁcially. Also, the longer the sentence, the less the concentration and consistency of the overall content composition. A structured approach (e.g., graph network) is proposed to keep the scenario cohesive and ) MULTIMODAL CONTENT REPRESENTATION In Metaverse, users create large amounts of multimedia content (e.g., images and videos) as well as text via an avatar. The multimedia data generated in this way expresses the user’s thoughts and experiences more than simple dialog. In order to effectively handle multi-modal content, there is an alignment method that converts data into different modal types and a method of expressing data of different modal types by integrating them into one representation []. Mul- timodal content enriches the content by adding information from the data of other modals and supplementing the lack of information of unimodal. By learning these cross-modal features, there is an advantage that intra-mode and inter- modal semantic relationships are utilized. In the Metaverse, multi-agents need to have different personas, as if each person has a personality, and multiple agents can interact with in different ways at the same time. It is difﬁcult to give the user a sense of immersion with a character who has a similar conversation every time. Metaverse needs a persona model that expresses various multimodal expressions (e.g., gestures and facial expressions) as well as conversations (e.g., Persona chat). Since spoken language understanding (SLU) uses information on persona without pitch lost in the process of converting a voice signal into text, it grasps a more accurate meaning that is not in the Although Metaverse users create a lot of user data, entity augmentation and persona generation are important because the data required for learning is relatively large to avoid a cold start problem and the sparsity of various NPC (non-player character) personas. In particular, unevenly user-generated data (e.g., conversation history and personal experience) is biased towards a particular subject until enough data has been An entity is a uniquely identiﬁed unit (e.g., the name of a famous place and person) and is associated with other entities and relationships. Entity-based expansion is a way to enrich user personas by increasing the number of entities. Methods for increasing the number of entities include generative models, reinforcement learning, joint inference, ontology, and multiple entity extension methods using intermodal []–[]. When tagged resources are scarce, there is a way further to extract entities through co-learning with other model data. Using the pre-trained model is a good way to extend the entity based on the balanced data. When creating Metaverse NPCs, novel approaches are needed to express persona and emotions that reﬂect the char- acteristic of worldview. Therefore, considering the scarcity of each modal, a data population is required to create a balanced persona used in various scenarios. Personas play an important role in giving users a sense of immersion by giving each character a personality in the Metaverse. In particular, it is necessary to implement characters with multi-personas like When constructing a conversational model, agents responded based on training data for correct answers rather than personality. Because such monotony makes it difﬁcult to maintain long conversations, some researchers proposed to maintain a consistent conversation pattern by introducing the persona concept. For the dialogue system to sustain longer and more human conversations, empathic dialogue systems consider personas [], []. However, dialogue data is insufﬁcient to create large persona representations. Personal Dialog is a large multi-rotational dialogue dataset based on sequential conditional GANs containing different characteristics of different speakers (e.g., age, gender, location, interest tags) []. Textual story creation focusing on persona is also proposed []. The conditional language model generates various forms (e.g., wiki, horror, humor) of sentences from preﬁxes without retraining []. ) MULTIMODAL ENTITY LINKING AND EXPANSION When deﬁning characters and entities by transforming the modal of various data, it is necessary to redeﬁne and extend the relationship between the contents through the connection between entities. In expressing the growth process of diverse events and characters in the Metaverse, causality is important in understanding the events and connecting them to the story. S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges Entity linking is the process of linking related entities based on the similarity between entities and the probability distribution of related contents. Methods for connecting enti- ties by structural learning include link prediction, nonlinear relationships, joint inference, and relationship classiﬁcation methods []–[]. In particular, the graph model shows how they are related to each other by using the relationship between entities, which are units of meaningful informa- tion. The connecting relationship is improved by explicitly modeling the interdependencies between object instances in the scene graph []. It also improves the performance inference by encoding global context and geometric layout. Research on graph models and graph convolution networks is increasing to extend the links of Since the Metaverse contains various worldviews, simply connecting objects is not enough, so the process of expanding and inferring links between entities is required. Inferring information based on given data is an important in enriching content. In particular, facts are derived by connecting hierarchically connected things based on causal relationships. Some studies used inference methods include variation inference, various modalities, ontology, emotion, Rather than listing events in the Metaverse, it is important to ﬁnd hidden relationships based on causal relationships between events and themes and construct a scenario line based on them. Unlike the text-based scenario, the Metaverse is more complex because it has to be conﬁgured in multi-modal and embodied environments. Each entity and relationship are used to organize events, and events must be organically combined to form scenario lines. Scenario lines construct the overall structure and serve as an index linking each event. Because it is not just a list of events, the entities and relationships in each event are linked together based on In order to compose a scenario line, it is necessary to con- nect events composed of entities and their relationships using a graph model. Events are divided into main events and sub- events according to their importance in the progress of the scenario. Scenario construction methods include continuous sequences, hierarchical structures, and the attention-based method by focusing on noteworthy content []–[]. When user behavior data in a scenario graph is accu- mulated over the lifetime of the avatar, to the concept of life logging. Key scenario topics are extracted with topic modeling and summarized personalized multimodal user data with generative language models. Yu and Riedl [] introduced a drama manager who personalizes user stories with plots and optimal sequences. Bolanos et al. [] described the visual life logging of storytelling by time slicing, summarizing, and retrieving important information. Li et al. [] proposed StoryGAN, a story-image sequence generation model that sequentially visualizes stories by generating one image sequence for each Scenarios expand by adding entities and linking the added entities with relation. Scenario lines form a skeleton and expand entities and links to events to create rich stories. Connections between events and other events are formed by relationships and are linked within a scenario. Entity expansion methods include translation embedding, attention, bidirectional inference, and relational inference []–[]. In the process of scenario graph population, modal conversion (e.g., text-to-video and video-to-text conversion) is used for multimodal integration. After pairing sentence nodes with images in a hierarchical approach, it adjusts the length of events through event summarization. The generated multi-mode scenario graph can be used to expand or collapse events. Each event is summarized as representative images with a multimodal language model. In an event-based extended scenario, as the scenario length- ens, inconsistencies between events occur, it is necessary to verify periodically whether the scenario does not conﬂict in concept. By instantiating the scenario graph, it hierarchically enlarges and contracts to verify that each event is organically connected and there is no contradiction. Scenario veriﬁcation is divided with a synthetic method based on grammar and a method directly verifying visualized graphs using human-deﬁned metrics []–[]. Human-deﬁned metrics are divided into a structural approach and a search-based approach. The structural approach evaluates the overall composition in which the scenario is balanced, while the search-based approach looks up speciﬁc facts with user queries to ensure that the scenario is well-formed without D. DISCUSSION AND OPEN CHALLENGE Users can also suffer from simulated motion sickness (i.e., cyber motion sickness) due to the imbalance of visual information obtained from human organs and eyes. There are focusing-displacement collisions and binocular-occlusion collisions, which may have side effects (e.g., blinking). There are other issues (e.g., physical fatigue, headset weight, movement injuries, hygiene issues from prolonged wear) and some side effects (e.g., thin motion sickness, vector motion sickness, eye fatigue, and seizures). In order to reduce side effects, postural stability and physiological measurement methods are used to measure the degree of motion sickness. In addition, adaptive optimization based on the measured values and stabilizing using stable cues have been proposed. Beyond that, there are alternative methods to minimize leading indicators, visual acceleration, and Cognitive stability and homeostasis are important for effective service in the Metaverse. Recently, in order to S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges support a more realistic sense, the scope of Metaverse has been expanded to smell and taste perception. On the other hand, interest in recognizing a complex sense by combining In order to process large amounts of real-time data, fast rendering and data analysis are required immediately. The speed of image processing is essential because the -degree ﬁeld of view is taken into account. Therefore, it is necessary to reduce the delay time through the expected tracking and measurement when rendering the object. Users are able to decide whether they want to organize their scenarios in a simple, concise summary format or as events in long, complex plots. The depth and length of the scenario are determined using modal and density transforms. Metaverse scenarios use hierarchical and causal relationships to organize events and adjust using techniques that adequately summarize the content of a sentence or paragraph. The resolution of the text can be interpreted in terms of the summary, the summary of a scene can consist of a panorama with multiple scenes connected, and it can represent an important timeline among multiple scenes []. In terms of scenario construction, studies to ﬁnd connections between scattered entities include clustering [], planning-based conditional branch- ing [], time-dependent index [], and visual analysis The completeness of the content is also an important factor in the Metaverse. For example, in Ready Player One, many interesting characters that dominated popular culture appeared, creating the illusion of going back to that time. However, the story development and probability were weak compared to the splendor of the visual. Natural interaction is an essential condition for increasing immersion in the Metaverse. It can reproduce the faces of friends and celebrities to enable realistic interactions the illusion of users with familiar and famous places. Temporary dissociation, concentration, and heightened enjoyment are important factors in the interaction, and emotions of control, curiosity, and intrinsic motivation are used. The target of interaction is mainly human, and hands are an important feature. Input devices are broadly divided into hand-held devices and non-hand input devices. Fidelity, proprioception, and egocentric view are important for interactions on physical devices. Since a -degree ﬁeld of view is used as the receptive ﬁeld for spatial recognition, a lot of images and distortion corrections are required for video processing efﬁciency. In order to reduce motion sickness and fatigue, visual and bodily sensory collisions and an alternative sensory method are needed. It also requires multimodal sensory perception that handles speech, gestures, The conversation is a basic approach to deliver user intent via voice recognition. In other words, language is used in various places because it concisely describes complex situations in an implicit sense. It is necessary to create a Metaverse environment in which understanding the situation through language, abstraction, QA, and translation. Miller et al. [] proposed ParlAI, an integrated framework for training and testing conversational models using multitasking training, data collection, human assessment, and online RL. Par- lAI performs various tasks in the same interface with dialog datasets (e.g., SQuAD, bAbI task, MCTest, Wik- iQA, QACNN, QADailyMail, CBT, bAbI dialog, Ubuntu, Languages are used in the RL domain as an effective way to deﬁne goals and abstract human-comprehensible tasks []–[]. Some agents classify instructions into a single skill level by mimicking human behavior []. When the agent is faced with an ambiguous situation, the agent clariﬁes the instruction intention through a multi-turn conversation with the Oracle []. Jiang et al. [] used a language that is ﬂexibly applied to the generalization of various goals, rapid training, and combinations as an abstraction to solve the difﬁculty of generalized abstraction AQM (Questioner’s Mind) agents ask more consistent questions to maximize information acquisition in task- oriented conversational systems []. Knowledge Graph A2C (KG-A2C) is a scalable exploratory method for inferring game states in a template-based workspace using linguistic behaviors and dynamic knowledge graphs []. Translation is an essential method in the Metaverse environment where people of various languages are gathered. Domhan et al. [] training on a large number of unpaired languages and a small number of language pairs to improve neural machine translation (NMT) performance. Humans facilitate efﬁcient adaptation and reason more abstractly by transferring knowledge across tasks. People communicate not only dialogue but also based on multi- modal information (e.g., facial expressions, gestures, and tone of voice). The method of handling each modal difﬁcult to handle multiple complex emotions, so multi- modal interaction is required. In general, multimodal has more information than unimodal and is advantageous for understanding the situation. Text and images in social media posts do not have the same meaning but instead have more complex meanings that intersect semantically []–[]. In particular, multimodal learning is most effective when the meanings of images and text are different. After the advent of Transformers, studies have been conducted to learn vision and language together and reduce learning from scratch using a pre-trained model. Zhou et al. [] proposed a uniﬁed vision language S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges dictionary training (VLP) model using a shared multi-layer transformer that ﬁne-tunes vision language generation. Since the Metaverse handles many things in the cyber world, a model that handles multiple tasks simultaneously is useful in the aspect of complexity. For such a model, knowledge distillation is used to make a small model that performs many functions and handles other modal types (e.g., Visual QA). Hessel et al. [] argued that multitasking is more complex than single-tasking because the multitasking model balances various tasks in limited expression. It is relatively easy to use for similar tasks but easily overﬁts when target domain data is scarce and has a different distribution. E2E methods are also used to perform various tasks effectively. Translatotron [] translated from voice input to voice output through a sequential process. Compared to the cascaded model, the E2E model has the advantage that most of the inputs can be utilized without data loss in the process. Translatotron interprets a foreign language, including its unique pronunciation and emotional meaning. Also, it has the advantage of responding in a voice form that reﬂects the prosody of the actual speaker. Qian et al. [] proposed an E2E modeling method SLU for a cloud-based modular dialog system (SDS), showing that it is effective in situations with The difference between the Metaverse and other general interactions is that the proportion of embodied interactions relatively high. While the required skills are similar to EQA and VLN, there is a difference in whether the subject is active or passive. While the purpose of VQA is to answer text questions about a given image, EQA (embodied question and answer) performs the task of analyzing sensor information obtained by an agent materialized through active exploration. For example, to answer a question about the color of a car at a distance, the agent actively moves, recognizes, and responds based on prior knowledge of the car’s location and path []. These EQA tasks have recently been extended in the form of conversations, where agents compensate by querying oracles for insufﬁcient information to perform the task []. The factor that differentiates embedded interaction from 2D-based methods is Exophora resolution. Anaphora reso- lution is the task of analyzing the word in the preceding sentence pointed to by a pronoun [], []. Anaphora and co-reference resolutions are used to infer cross-references in questions and conversations [], []. In short sentences, implied conversations, anaphora resolution is needed to understand the context of the conversation. Recently, such anaphora has been widely used in multi-modal content (e.g., video) and SNS services (e.g., Twitter) beyond simple sentence-based analysis [], []. Exophora resolution maps the meaning of Co-reference resolution and Anaphora resolution used in language to 3D space. People communicate information in a non-verbal form by pointing to an object instead of language. When a user points to a speciﬁc location through a ﬁnger, it becomes an intended instruction. In the case of exophora resolution, speciﬁc instructions are performed in terms of multimodal interaction, including motion and speech, whereas anaphora simply links meaning between texts. Heinrich et al. [] proposed Embodied Multi-modal Interaction in Language learning (EMIL), a neurocognitive model that reﬂects in vivo- inspired mechanisms (e.g., an implicit adaptation of time The process of Metaverse implementation is divided into a design phase, a model-training phase, an operation phase, and an evaluation phase. The design phase considers goals and concept design, development time and cost, risk estimates, constraints, user scenarios, scope and requirements, and feasibility of implementation and evaluation. In the model- training phase, data analysis, user modeling, scientiﬁc methodology, iterative learning, and parameter tuning are performed. The operation phase considers system consider- ations, simulations, job scheduling, network environments, and prototype demonstrations. The evaluation phase deals with content ﬁdelity, the authenticity of interactions, imple- mentation feasibility, and failover. This survey covers three types of multimodal inference, RL-based approaches, and lifelong learning for Metaverse training models. In addition, it is necessary to consider multi- agent optimization, integration optimization, and operational considerations from the perspective of Metaverse service Humans do not only interpret the meaning of utterances when communicating with others. When information is given from the cognitive model, it interprets its meaning, combines it with its knowledge, and inferences its inten- tions. Verbal ambiguity is compensated to determine the speaker’s underlying intentions based on direct or indirect representations of the surrounding environment. For example, emotion recognition, the initiator of emotional interaction, uses multimodal fusion to compensate for the lack of context in textual information []. Multi-modal models do not always outperform single-mode models, so they should be utilized according to the situation. Zhang et al. [] used late fusion to explicitly examine the impact of each function by considering three types of visual, spatial, and semantic. Liang et al. [] proposed Multimodal Local- Global Ranking Fusion (MLRF), relative sentiment analysis for complex combinations of visual and acoustics. Rather than simply classifying emotions as scalar values, the ranking was performed after measuring the degree of increase or decrease in emotional intensity for partial video segments. The advantage of the pre-trained model is that it simpliﬁes the task with E2E and does not have to learn from scratch. S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges Recently, DialogGPT and Vlbert are proposed to implement dialog and visual-language tasks more conveniently. Large- scale pre-trained language models (PLMs) (e.g., Bidirec- tional Encoder Representations from Transformers (BERT), GPT-) are used for downstream tasks by applying ﬁne- tuning and few-shot learning []. Sun et al. [] propose MobileBERT to compress and accelerate the BERT model. MobileBERT uses the knowledge transfer student model from the teacher BERT large model. Brown et al. [] proposed GPT-, a  billion parameterized autoregressive model that applies several pieces of training runs without any gradient updates and ﬁne-tuning for downstream tasks. Tan et al. [] proposed contextual mapping of language tokens and associated images with vokenization and multi- modal alignment. It is applied to a relatively small image caption dataset using the generated model. T5, which integrates text tasks into one model, is proposed to handle translation, question-and-answer (QA), etc. []. Video pre-trained models (VPMs) that contain multimodal data of vision and text are effectively used in low-complexity downstream tasks (e.g., VideoBERT [], ViLBERT []). VPMs are used for answering visual questions, common sense reasoning, reference representation, and caption-based image retrieval. BERT-based VPM performs vector quantiza- tion of video data and trains bidirectional joint distributions for visual and verbal token sequences. Some studies give a sense of presence in Metaverse. Domain knowledge understanding provides a more detailed response based on facts. Spoken language understanding deals with the user’s tone and emotions. Acoustic signal understanding recognizes and generates sounds from the surrounding environment. Reasoning (e.g., multi-hop reason- ing, relational reasoning, and graph reasoning) derived new facts through prior knowledge, background knowledge, and environmental factors given in the current situation. Multi-hop inference in graph neural networks (GNN) has been used to generate new knowledge about vision and language []–[]. Because graphs, along with KB, act as a repository of knowledge, to effectively utilize encoding, sampling, and utilization in visual language interactions. GCN is a representative model for training representations of attribute graphs. Graph inference trains ﬁxed representations of entities in multiple relational graphs, which are generalized to infer invisible entity relationships during inference. Various approaches are proposed to improve graph reasoning [], []. Multi-agent RL, Imagination-augmented RL, and Language- grounded RL are utilized in Metaverse because RL is suitable for action in a situation without prior learning. Multi-agent RL provides realistic NPCs by causing collaboration and dis- putes among various agents. Imagination-augmented RL has the feature of rapidly stabilizing without enormous training data, and language-based RL is used for conversation. Technically, RL is a method to achieve an objective goal by determining the behavior that will receive the maximum reward based on the state received from the environment. is divided into model-based RL and model-free RL according to the existence of a model for a task. It is also divided into a value-based method and a policy-based method according to the training method. The on-policy method trains an algorithm using the deterministic output of the target policy, whereas the off-policy method indirectly creates and trains a stored distribution. Compensation methods (e.g., episodic memory, world model, and language-based RL) have been proposed to solve the problem of inefﬁciency and sparse rewards of RL sampling. Furthermore, more efﬁcient approaches (e.g., ofﬂine RL and control RL) are emerging to solve fundamental problems (e.g., sample inefﬁciency, unstable training). Unlike traditional off-policy RL and model-based RL, ofﬂine RL uses only pre-collected training data, not online results. Ofﬂine RL shows reliable learning with batch training and good performance in a closed-loop RL methods are steadily growing through knowledge sharing, memory, abstraction, and language bases. The Diversity all you need (DIAYN) model learns useful skills without a reward function, just as humans navigate the environment without supervision []. DIAYN acquires skills by maximizing information-theoretic goals using a maximum entropy policy. Laskin et al. [] proposed Con- trastive Unsupervised Representations for Reinforcement Learning (CURL) that utilizes the advanced capabilities of raw pixels using contrast learning and out-of-policy controls. Xavier et al. [] proposed a watch-and-help (WAH) model that uses a single demonstration of an agent performing the same task to understand the task’s goal and work with a human-like agent to solve a problem. Life-long learning is meaningful because it builds experience points over a long period in a sustainable Metaverse. For such life-long learning, a method that effectively memory existing data and use it at an appropriate time is required. Most solutions and services have a constant cycle. In order to apply lifelong learning to Metaverse, it is necessary to consider how to maintain long-term service. On the other hand, the key to life-long learning is how to handle the catastrophic forgetting that most neural net models have. Relationships between multiple agents are divided into collaborative, competitive, and oracle relationships. In order to effectively utilize these relationships in multi-agents, it is necessary to introduce a mental model (e.g., the Theory intrinsic motivation, and heterogeneous competition). Based on the concepts and experimental results of psychology and neuroscience, there have been attempts to solve the problem of neural networks. In particular, the inductive bias, and intrinsic motivation S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges Will et al. [] used dopamine’s reward prediction error theory to explain rich empirical phenomena. They provide an integrated framework for understanding the representation of rewards and values in the brain. They describe that the brain represents possible rewards as a probability distribution rather than a single scalar, and various future outcomes are spontaneously expressed in parallel. Episodic memory tracks functional and structural interactions between brain regions, particularly the hippocampus []. Episodic memory, unlike semantic memory, is a descriptive memory that contains information related to the time and place of acquisition. Gradient episodic memory reduces forgetting by transferring previous knowledge to evaluate model training on continuous data []. Oudeyer et al. [] explained how psychology and neuroscience conceptualize curiosity and intrinsic motivation as intrinsic rewards for the brain’s novelty, complexity, and information scale. Rabinowitz et al. [] designed a mind neuron theory network, ToMnet that uses meta-learning to observe behavior to build agent models. Melhart et al. [] investigated how the emotional mind theory of gameplay inﬂuences behavioral recognition, performance, and frustration behavior in facial Cooperative multi-agent RL requires a distributed policy but has limitations in coordinating agent behavior in complex environments. Agents reconstruct each other’s observations to generate common knowledge in distributed, collaborative multi-agent operations. It is also essential to study how to believe from the perspective of other agents and humans through collaboration []. Humans share potential minds (e.g., beliefs) and these social methods are important for recursive reasoning about the potential consequences of other In order to effectively operate multiple agents, various optimization methods are proposed. Gated propagation networks improve training with attention and gating on graphs that propagate messages between prototypes of different classes and update them in memory of different classes []. Multimodal MAML modulates meta-trained prior parameters to enable fast adaptation and improved training on multimodal distributions []. An integrated platform is needed to handle various modals and various events and interactions. Racanière et al. [] proposed an I2A (Imagination-Augmented Agents) for deep RL combining model-free RL and model-based RL. ZEPETO is a platform that is completely provided in the form of a service, and Unity provides more freedom in which developers create the world they want. Continuous service through human-centered design and multi-modal interaction is important from an artistic point of view and a scientiﬁc point of view based on design philosophy. Meta RL based on few-shot learning is used because real-time performance is poor to analyze service. Graph RL using the structural characteristics of knowledge is also attracting attention. Because planning is essential to perform more complex scenarios, there are many studies on Planning RL. In order to provide stable service on the integrated platform, it is necessary to cope with network bandwidth and failure response physically. In addition, measures against social and politically sensitive issues (e.g., sanctions and hacking) are required. Most of the research on Metaverse is aimed at marketing and investment purposes, emphasizing social utility. The domains where Metaverse is popularly serviced are games and some ofﬁce applications. Huggett [] argued that there is a separation between the present reality and virtual reality of virtual heritage and conducted a study of existence and realism within virtual reality. Skarbez [] introduced mixed reality, real-world modeling, and real-world modeling. For better Metaverse applications, an approach is needed to model and distinguish the differences and the same points between virtual reality and reality. Metaverse is being serviced in various forms of application. The simulation starts with a game and is also used for social phenomenon research and marketing simulation. Because it has an educational effect through simulation, it is also used for education and museum visits. Simulations depicting real- world tasks are a universally available application in the Metaverse. General simulation is solution-dependent, but the simulation of Metaverse is performed in Metaverse, so it is different from general simulation. Maharg and Owen [] conducted a study on application simulation for educational purposes. Siyaev and Jo [] conducted a study on virtual assets and workﬂow control using aircraft engineer voice commands. In the case of a virtual environment based on the real environment, exaggeration and the intention of the creator can be included in the process of describing the environment in Metaverse. Shi et al. [] studied the difference between the virtual and real environments by evaluating the agreement between the ﬁeld survey and VR Gordon et al. [] proposed a hierarchical interactive memory network (HIMN) consisting of a factored set of controllers and operating at multiple levels of temporal abstraction. They also introduce IQUAD V1, which simulates realistic environments of indoor scenes that can be conﬁgured with interactive objects. Qiu et al. [] proposed an object- driven visual search algorithm, MJOLNIR (memory-utilized co-hierarchical object learning for indoor room navigation), that learns how to associate objects with prior knowledge. Li et al. [] proposed a MIND (Mental Imagery eNhanceD) module to model the dynamics of the environment and S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges create objects for a better understanding of the implemented agent. Tamari et al. [] described that natural language in cognitive linguistics (ECL) is inherently executable and driven by metaphorical mappings and mental simulations to schemas learned through hierarchical organization and Games are the most common platform in the popularization of the Metaverse. In addition to simply focusing on interest, there are ways to approach to simplify difﬁcult tasks through games. As much as payment and personal information are widely used in Metaverse, a game based on blockchain technology has been proposed []. Hide and Seek is a simple yet effective simulation environment for multi- and scenes from an egocentric perspective []–[]. Baker et al. [] found that agents create a self-supervised automation curriculum that drives new strategies of multiple stages in a multi-agent competitive environment (i.e., hide and seek). Stanica et al. [] Introduced Neurorehabilitation Exercises Using Virtual Reality (INREX-VR), an immersive neurorehabilitation system using virtual reality. They capture real-time user movements in gamiﬁed environments and execute complex movements to encourage self-improvement online solutions in B2B solutions and conferences, some companies introduced and supplemented the ofﬂine concept. In this way, the sound occurring in the ofﬁce and physical elements (e.g., desks and conference rooms) is given a sense of space. Representative examples of ofﬁce applications include solutions (e.g., Branch, Gather, and Teamﬂow) and use spatial audio technology to provide speech and footstep sounds according to distance. The Branch is given a game element that offers virtual currency and experience. Teamﬂow has the advantage of using work-related tools (e.g., Because avatars change skin color and gender as desired, they have the advantage of reducing preconceived notions about social discrimination in conversations. These embodied avatars are more advantageous for simulating social problems than in the form of surveys and role-play. Papagianni- dis et al. [] conducted a study on the impact of corporate social responsibility focusing on ethical and policy-related issues. De Decker et al. [] introduce the study on the process for solving complex social problems was conducted using Metaverse. Smart et al. [] explained the important characteristics of social change in the Metaverse and future The online requirements for cultural life (e.g., museums and performances) are gradually increasing. Although the limited capacity and time constraints of an ofﬂine concert hall are solved, there is still a lack of differences in texture and ﬁne detail that can be felt ofﬂine. Tang [] evaluated the immersive service using Metaverse for educational library orientation. Choi and Kim [] studied how visitors experience museums by combining beacons and HMDs. Hazan [] explored how museum social and cultural Economic activity is an important content in the Metaverse. It creates an ecosystem that continues economic activity by consuming clothes and goods provided by the production company and producing and selling them with other users. Metaverse is a virtual world to predict the future by reﬂecting the characteristics of reality realistically. Kaplan et al. [] dealt with how companies see their differences from other social media and utilize their potential. Cagnina et al. [] conducted a study on the business model of a company in Virtual Worlds and Second Life. Papagiannidis et al. [] described Second Life’s take on this retail theater experience. ANoghabaei et al. [] covered industry trends in AR and Audiovisual-based education is an important application of Metaverse with a high potential for popularization. Experi- ential education is important because what you see in writing and how you feel while experiencing it are different. For example, radiation is difﬁcult to experience, so you may pre- conceive that it is simply dangerous. Through the Metaverse, it is possible to see the educational effects that are considered while analyzing and experiencing radioactivity technically and scientiﬁcally in Metaverse []. Sung et al. [] compared the level of immersion and three learning outcomes (learning attitude, enjoyment, and performance) based on facial electromyography by comparing marketing students with existing static video presentations and showed that the meta world method is effective in education. Kemp and Livingstone [] analyzed the advantages and disadvantages of a multi-user virtual environment for educa- tion, and Collins [] studied how to access, interact, and gen- erate information in higher education. Templeton et al. [] addressed practical and educational considerations for learn- ing teachers, Suzuki et al. [] conducted a study on mutual collaboration in learning IoT. Metaverse is used in PBL, a problem-based learning method as an educational framework [], []. Barry et al. [] evaluated the quality of instruction in the PBL task based on the increase in the number of blinks that made students’ emotions unstable and difﬁcult questions. Khan et al. [] proposed safety training for children in the outdoor environment with VR, Kinect sensor, and the Unity game engine. Muhammad et al. [] introduced the effectiveness of handheld marker-based AR in the aspects o performance, motivation, attitude, and behavior S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges For scene and object recognition, object recognition was the name of the game character and the performance and status of the motorcycle was displayed on the scope. Sound and speech recognition and synthesis were not speciﬁcally addressed. Basically, it seems that the recognition and expression of dialogue is a domain with rapid technological development, and it is assumed that it will be in a free state in the near future. Likewise, since it is an animation, they did not deal with speciﬁc motion rendering. On the other hand, scene and object generation is used in many places. A holographic map that converts to the actual background while zooming in, the rotation of surrounding buildings before the racing starts, and the effects that look mysterious as a light effect in a dramatic situation are representative examples of scene generation. There were detailed object generation methods (e.g., hair that changes according to feeling), UI interfaces for avatars (e.g., Jarvis), musical instruments played without a player, and textured In a museum, personal photographs, home video recording, surveillance, and nanny cams are expressed in a single multimodal content representation. Metaverse’s persona data generation is approaching tall, beautiful, scary, different live-action, and cartoon without restrictions. The protagonist has a virtual best friend who in the real world. His unrealistic colors and shapes (e.g., gray skin, machine body, light source clothing, and ﬁsh) of the avatar expresses the strengths of the virtual world well. Another characteristic of the movie is the thorough anonymization between the avatar and real self. What happens in the Metaverse is considered as a form that excludes direct inﬂuence on the real world. Representative virtual NPCs are shown as a simulation curator who helps introduction and events and an avatar of a creator who progresses the story. Multimodal entity linking is used to create 3D virtual experiences with personal photographs, home video recording, surveillance, and nanny cams. In the case of scenario generation, an important theme runs through the entire ﬁlm. There are many sub-quests (e.g., collecting coins in the event space, planet12) and the main quest that The entire scenario line features the death of a respected game creator, as well as massive rewards for Easter eggs as missions. The creator’s avatar guards three magic gate keys in various places, and the avatars do an adventure to ﬁnd the keys for the Easter eggs, which rule in Metaverse. The avatars carry it out in the Metaverse through missions that require a reasonable level of common sense and reasoning. It makes you more immersed in the scenario by showing things that are difﬁcult to experience in reality (e.g., announcing the start from the ﬂames of the Statue of Liberty and ﬂying subways). The scenario is populated with the visual experience similar FIGURE . Use case of Metaverse movie, Ready Player One. D. DISCUSSION AND OPEN CHALLENGE Component models for modal conversion are developed into various forms, from text-to-image conversion to image-to- image translation and video-to-video synthesis. Although the technology for generating the elements of the textual scenario has matured, the integrated research related to the creation of multimodal applications is insufﬁcient. Along with the study of these transformations, there is also a need for studies on E2E learning that simpliﬁes the integration of modules to reduce the complexity of creating multimodal applications. In addition, values, beliefs, attitudes, memories, and deci- sions are valuable concepts to expand in-depth applications through psychological and neuro-linguistic programming. The utilization of Metaverse in science ﬁction (SF) is important not only for CGI (computer-generated imagery) but also for UI design. The futuristic UI shown in Minor Report wearable, G-speak, Iron Man HUD, Oblivion, and Enders games provide visual insight for the Metaverse UI. We discuss what technologies were utilized based on the taxonomy proposed in Section  in the Ready Player One movie, which is always referred to when talking about the Metaverse. In addition, we do a case study about Roblox, a representative game of the Metaverse. Finally, looking at the recent research results of Facebook Research, the technical possibilities and approaches of Metaverse are summarized. A. METAVERSE MOVIE: READY PLAYER ONE ) PHYSICAL DEVICES AND SENSORS For Head-Mounted Displays, holographic HMDs and goggles-type HMDs were used in the movie as shown in Fig. . The holographic HMD is mounted on the neck and plays the role of displaying it on the front. Gloves with sensors that wrap around the hand are used for hand-based input methods. Non-hand-based input methods show in full- body suits that can differentiate and deliver the impact of push, punch, and gunshot. In addition, the shock sensor attached to the chest plays a role in delivering the shock from the virtual world to the human body. As an indirect assistive device, a translucent display tablet used by children at school and a tablet with an expandable screen have emerged. The treadmill comes out as a motion input method, and walking and running are distinguished, and a safe environment is considered by ﬁxing it with a belt. S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges to the real ones in the virtual environment while converting the viewpoint and speed. Scenario evaluation shows whether there is any contradiction while combining the events before and after about the relationship between the creator and his Although language and multimodal speciﬁcally mentioned, it assumes an organic combination with no synchronization and awkwardness in processing 3D stereoscopic screens and conversations as multi-task interactions. Showing the number of kills and damage by overlaying them on the gun, passing money, and throwing objects in the air shows the advantages of embodied interaction. A car is carried as small as a car key and be put into virtual space as an inventory concept. It gradually expands from a small icon to a real-sized car when taking out the car, giving visual pleasure. User interaction with NPC is also considered. The AI robot helps the protagonist by using a search system in the museum. Immersive OASIS connection video plays a role in distinguishing the real and the virtual. The last motion to exit the Metaverse is used as a pose to take Basic simulations (e.g., ballet, boxing, piano, dance, and tennis) appear at the beginning. The opening video shows that things that are difﬁcult to experience in ordinary life (e.g., hang gliding, waves in Hawaii, skiing at the pyramids, climbing with Batman and Everest, a planet-sized casino, divorce, and marriage) are possible. In the Metaverse, Gregarious games, Minecraft, and 3D pinball are mini- games, and avatars receive reward coins according to their level and risk. Avatar acquires coins when a car and a person breaks in Metaverse, but visual effects and damage realistically apply to vehicles collide. Through a scoreboard with rankings in the Metaverse, the intermediate process and results of the game are shared. Regarding the Ofﬁce, we expect an organizational approach of a company when looking at the employees of IOI companies who work in an independent space in a company with the shape of an avatar. It also talks about the possibility of an organized group with a commercial goal as an interesting company appearing inside the Metaverse and going to a racing game as if it were a job. From a social point of view, class according to grade, disconnection from children due to game participation, and side effects of anger caused by immersive game participation are described. There is a view of blocking from the real world, which is shown by anonymizing names with numbers instead of names. On the other hand, some example shows that communicate through the interface between the real world. Even in the Metaverse world, a marketing singularity sells ofﬂine items (e.g., suits) and is appropriately used in game items. Although it is in the Metaverse that mimics the real world, unrealistic control function items in the Metaverse FIGURE . Use case for Metaverse game, Roblox []. (e.g., the time turning item) are also used in a balanced way that does not threaten the world view. The appearance of education suggests that the form of education will not be much different from what it is now, although a translucent ) DISCUSSION AND OPEN CHALLENGES Player Ready One shows negative aspects of the Metaverse (e.g., surrogate exam, taste cheating, and mirroring). The problem of over-addiction is explained in the appearance of upgrading a suit with the money to be paid for the rent due to the virtual world and excessive immersion. Metaverse is based on separation from reality but depicts the fact that virtual damage is done to the real world. Finding the owner of the avatar in the real world and jumping out the window in anger over defeat are mental problems in the immersive Metaverse. The appearance of falling off a chair and falling backward is expressed as an example in which the Metaverse Metaverse implementation is not described in detail in the movie except for the concept of life-long learning for the museum scene because it is a technical detail. By visualizing the avatar as a hologram in the real world, they showed it is possible not only in the forward direction (i.e., from reality to the Metaverse) but also in the reverse direction (i.e., from the Metaverse to reality). Eating, sleeping, and bathroom breaks are seen as new expandable possibilities in the sense that they Roblox served by two-thirds of - years old in the United States and is a representative game of Metaverse with an MAU of  million []. Roblox is also used to develop simulations of urban environments to describe experiences that incorporate the realization of virtual paths to the city’s sculptural heritage in the classroom, as shown in Fig. . Students were able to understand and integrate Santa Cruz’s sculptural heritage to create their own interactive world with Roblox []. Creatively interpreting legacy in both formative and programming is a good approach. Although norms between education and entertainment have often been regarded as two separate worlds, Roblox is used as an educational tool in the classroom from the perspectives of motivation, problem-solving, and STEM []. ) PHYSICAL DEVICES AND SENSORS Roblox supports Oculus Quest  and HTC Vive for 3D HMD. VR has a gyroscope, display screen, and built-in audio S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges system that provides a VR experience as an independent device that is different from the Google Cardboard method using a smartphone screen. The Quest  is compatible with PlayStation VR and does not require a gaming PC, but a VR-capable PC is required to run Roblox. It is the cheapest VR head for Roblox, and it has a vast resolution of  ×  pixels per eye, but the front of the device is heavier, which is inconvenient. With full VR support, users play games like Skyrim, Half-Life: Alyx, and No Man’s Sky in Roblox. The HTC Vive Pro has a resolution of  ×  pixels and has a built-in audio system. There is additional padding throughout the device to share weight and balance, but the HTC Vive Pro only works via DisplayPort. Roblox supports VR devices and supports VR for some games, but it is still limited. In addition, most devices are limited to HMDs, so they lack the versatility of a tactile and pressure sensor that a normal Metaverse would consider supporting. Since the main customers are a young generation, they seem to focus on simple forms over complicated ones. The SW used in Roblox is Lua and Roblox studio. Lua is a small interpreter language with a capacity of several hundred tens of KB. The script is a programming language that can be executed line by line and is a tool that creates events that occur in the game, physics engine, text output, and screen effects. It was developed with the goal of being a lightweight scripting language with a clean syntax that is easy to embed Roblox Studio is the ofﬁcial free utility software for creating custom games for Roblox. Users conﬁgure various game worlds and servers (e.g., mini-games, obstacle courses, and role-playing stories). Its characteristic is that even low- quality games are made by own hand and enjoyed with friends. Because operation on low-end devices (e.g., mobiles) cannot be played in high-end games, they have an experience with lag, skin color errors, dialogue control errors, airplanes malfunction, etc. UnityML has the advantage of being able to link and use various recognition methods in a 3D environment, but Roblox is often composed of simple and lightweight forms, so that part is insufﬁcient. It is also used for the misuse of items that can disguise the On the other hand, the strength of Roblox is that various users can easily create new games. Although it is relatively simple, it presents a new perspective with various and novel approaches. However, each game is centered on a single story and lacks the depth of the story because it does not have an elaborate plot. Sometimes, the story is similar, and the story development is not stable. When there is an authoring tool (e.g., multimodal story generator) to make and evaluate plots, users easily create more in-depth content and games. The Metaverse trading system supports user exchange with other players for dollars, so a connection with the real world is also considered. Premium service provides a differentiated service that makes shirts and pants, sells them free, and sells them at a price. It also reinforces the interaction by providing an online hangout-concept space called a separate party place. There are various auxiliary methods (e.g., facial expressions, clothes, motions, and words) to express their feeling in Roblox. However, there is room for improvement in real- time and tactile interactions. Special attention is required for interaction because children spend a lot of time. It supports multiple languages but has a low level of translation quality. One of the most problematic for Metaverse commercializa- tion is stable operation, especially 3D rendering, for many concurrent users. From an operational point of view, there are problems with hacking, extortion, and server down. Management and efforts are in place to ensure user safety (e.g., prevent profanity, review on image uploads, parents prohibit chatting, more than , administrators), but as the scale grows, the number of users’ improper behaviors increases. Games administrator build their own reporting systems for these shortcomings and sanction them. On the other hand, excessive restrictions and privacy authority are also a problem. There is a privacy issue where the management can censor personal messages and know the There are many young users, so the game effects and scenario complexity are low, but it is diverse and novel. It is an online game creator system in which most of the content is produced by amateur game creators. User-generated content is an avatar accessory created by a user, and it is an item that can be created when a user has a reputation within the community and is an expert who handles modeling programs well. When users satisfy the conditions and get Roblox’s certiﬁcation, users get permission to create items using mesh, and users have to pass Roblox’s review. It doubles the elements of the game with limited items but also creates a counterfeit UGC and copies unique items to bring about a deﬂationary effect. Roblox is a game playground. Since there are not many games that elementary school students can play easily and comfortably, it can be seen as an imaginative game that can be seen in playgrounds. Since game items and passes are possible to break the game balance, the balance is important for commercialization. A concert called One world together at home is also opened as an application. It is used as a tangible connection medium to generate revenue through the production of ZEPETO items and to deliver from the real world to the virtual world using Roblox currency. There are phenomena that are seen in general society, hyperinﬂation following the abolition of Ticks. S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges FIGURE . Recent researches of Facebook research. ) DISCUSSION AND OPEN CHALLENGES Roblox supports VR, but non-VR games account for a signiﬁcant portion, and there is a possibility that it will develop into a more advanced form based on a large number of subscribers. Roblox is well known to the younger generation, so children can learn to code and make friends by taking Roblox coding classes and camps. However, contents because there is  million game content despite the overall user acceptance level. This management problem is problematic in that the primary user class is relatively C. METAVERSE RESEARCH: FACEBOOK RESEARCH Based on the papers published in Facebook Research from January to June , we classiﬁed each paper into the taxonomy deﬁned in Section  and summarized our approach in terms of Metaverse utilization, as shown ) PHYSICAL DEVICES AND SENSORS One of the hallmarks of Metaverse using a head-mounted it sees the world from an egocentric perspective. Most video processing uses third-person video S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges data sets, so egocentric video data is not enough. Third- person view data is not directly available in the Metaverse due to the inconsistency of the viewpoints, so an approach that transforms it into an egocentric video model is required. Li et al. [] generated a model that exploited knowledge distillation loss during pre-training to obtain both the scale and diversity of third-person video data, as well as representations with prominent egocentric properties. Xian et al. [] presented a method for learning a spatiotemporal neural irradiance ﬁeld for a dynamic scene that enables preview rendering of the input video. Using the scene depth estimated in the video, they constrained the time-varying geometry of the dynamic scene representation and presented a single global representation of the contents of individual frames. Generating expressive camera motion technology is difﬁcult because it requires editing of several control parameters that are not intuitive for users. Bonatti et al. [] developed a data-driven framework for editing complex camera position parameters in semantic space. They constructed a semantic control space by analyzing the correlation between technicians based on the study of ﬁlming guidelines and human perception. On a physical keyboard, the resistance of the keys prevents erroneous input, but in Metaverse, it is needed to isolate spurious input events when typing with a virtual keyboard. Foy et al. [] showed three alternative co-activation detec- tion strategies with high accuracy. They developed StickyPie, a marking menu technology that enables scale-independent marking input by estimating intermittent landing positions. They identiﬁed issues inherent in eye movement control and current eye-tracking hardware, including erroneous selection activation, while reducing workload and eyestrain. Natural hand manipulation is a task that requires complex ﬁnger manipulation to adapt to the shape and task of an object. Zhang et al. [] proposed a generalizable hand- object space representation combining voxel occupancy and global object shape with local geometric details to the nearest sample. Hand social contact is essential for social interaction and communication and reduces anxiety and loneliness. Rognon et al. [] Introduced mediated social contact that conveys indescribable emotions (e.g., love, empathy, reassurance), allowing devices to transmit haptic signals and physically interact at any distance. c: NON-HAND-BASED INPUT METHODS Research on input devices using wrist motion without directly attaching to the hand is also increasing. With the growing in vibrotactile feedback in wearable wristband devices, Chase et al. [] used information transfer as a metric to explore the signal variation space within a single vibrotactile actuator (e.g., frequency, amplitude, and modulation). Typical control systems rely on digital on/off the degrees of freedom available when designing haptic experiences, allowing only inﬂate/decrease at a set rate. Stephens-Fripp et al. [] presented an alternative system in which analog control of the pneumatic wave proﬁle can be used to determine the optimal wave proﬁle. The attack and release proﬁle have been altered to create a more pleasant pulsating sensation at the wrist and a more lasting sensation of transmitting movement around the To accurately estimate 3D human movement, both kinematics (i.e., body movement without physical force) and dynamics (i.e., movement with physical force) must be modeled. [] presented a SimPoE, a simulation- based approach for 3D human pose estimation that inte- grates image-based kinematic inference with physics-based dynamic modeling. To obtain accurate pose estimates, a meta- control mechanism was used that dynamically adjusts the character’s dynamic parameters according to the character’s state. Neverova et al. [] jointly learned the geometry of several categories of deformable objects to learn integrated dense pose predictors for several categories of related objects. It has symmetric inter-category periodic consistency and a new asymmetric image-category periodic consistency and has improved performance over methods for 3D shape matching without manual annotation of inter-category Lucas and Kozary [] focused on the basics of teaching computers to think like humans when making decisions about visual content that are most interesting and important to human viewers. Computers see colors as numbers rather than meaningful parts of an image, and textures see numbers rather than meaningful hard and soft parts of an image. Some parts are similar to human perception, but there are also other parts, so the difference between humans and computers is an a: SCENE AND OBJECT RECOGNITION The hard inductive bias of CNNs allows for sample- efﬁcient learning, but at the expense of potentially lower performance limits. Vision Transformers (ViTs) rely on more ﬂexible self-attention layers and perform better than CNNs in image classiﬁcation. However, expensive pre-training on large external data sets or distillation of pre-trained convolutional networks is required. d’Ascoli et al. [] introduced gated positional self-attention (GPSA), a form of positional self-awareness equipped with soft convolutional induced bias. The use of cropping can bias large objects to be clipped or omitted, as described in Lorenzo et al. [] proposed a new crop recognition bounding box regression loss (CABB loss) that facilitates prediction to match the visible part of the cropped object. In response to the disproportionate distribution of object sizes, they introduce a new data sampling and augmentation strategy that improves generalization across scales. Cheng et al. [] updated S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges the standard evaluation protocol, for instance, and panoptic segmentation tasks by proposing Boundary AP (Average Precision) and Boundary PQ (Panoptic Quality) metrics, respectively, based on Boundary IoU for image segmentation The spatially deformed spatial resolution of the retina is utilized for foveated video compression for immersive video requiring large bandwidths of high spatial and temporal Entropic Differencing), a Full Reference (FR) centric image quality evaluation algorithm for centric video compression. Xiong et al. [] presented a multi-view pseudo-labeling approach using complementary views in the form of shape and motion information for semi-supervised learning in the video. By acquiring pseudo-labels from unlabeled videos, more robust video representations were learned than purely It is also necessary to use text information as well as video. Huang et al. [] proposed Multiplexed Multilingual Mask TextSpotter, an E2E approach, for end-to-end education and scalable multilingual multi-purpose OCR system. They kept the integration loss of performing script identiﬁcation at the word level and processing different scripts with different recognition heads while simultaneously optimizing script identiﬁcation and multiple recognition heads. In the past, scene text-based inference separated from OCR systems was difﬁcult due to the lack of ground-truth text annotations or scene text detection and recognition datasets for real images. Singh et al. [] introduced a TextOCR for detecting and recognizing scene texts of arbitrary shape with 900k annotation words collected from real images in the TextVQA Since the Metaverse assumes a 3D environment, many 3D-related skills (e.g., fast rendering and few-shot learning) are required. Sodhani et al. [] proposed an open-source OpenNEED consisting of a large-scale, high-frame-rate non- eye (head, hand, and scene) and eye (3D gaze vector) data set. They proposed a robust eye tracker design considering non- eye sensors to study the relationship of head, hand, scene, and gaze and apply spatiotemporal statistics to gaze estimation. Henzler et al. [] proposed a new neural network called warp-conditioned ray embedding (WCR) that focuses on training a model on multiple views on a large collection of object instances to learn a deep network that reconstructs in 3D given a small number of images. Ren et al. [] introduced WyPR, a weakly supervised framework for point cloud recognition that requires only scene-level class tags as a director. They proposed to solve jointly by combining point-level semantic segmentation, 3D proposal generation and 3D object detection, and self- and cross-task coherence Liu et al. [] proposed an Unbiased Teacher to identify the pseudo-label bias problem of SS-OD (Semi-Supervised Object Detection). It is a simple but effective approach to train students and progressively develop teachers mutually beneﬁcially jointly. Chen and He [] used negative sample pairs, large batches, and momentum encoders to avoid solu- tion decay and showed that the gradient stopping operation plays an essential role in preventing decay. Tian et al. [] studied the nonlinear learning dynamics of uncollated SSL in a simple linear network where SSL with only positive pairs avoids expression decay. They investigate conceptual insights into how the disjoint SSL method learns, how to avoid expression collapse, and how several factors (e.g., predictor networks, stationary gradients, exponential moving averages, and weight reduction) work. b: SOUND AND SPEECH RECOGNITION Metaverse runs in a variety of places, from a relatively quiet house to a space where a variety of people gathers. Donley et al. [] proposed Linearly Constrained Minimum Variance (LCMV), an automated solution for multi-channel signal enhancement to improve voice communication in a noisy environment. They use the beamformer to estimate the relative source contribution of each source in the mixture and then used to weight statistical estimates of the spatial properties of each source used for the ﬁnal separation. It allows instant selection of desired and undesired sources. Furthermore, it improves multi-channel speech enhancement for dialogue, aiming to extract clear speech from a noisy mixture using signals captured by multiple microphones. Panagiotis et al. [] applied a graph neural network (GNN) to ﬁnd the spatial correlation between various channels and integrated it into the embedding space of the U-Net architecture with the graph convolution network (GCN). Array Renderer (ReTiSAR) to analyze the sensor’s own noise propagation through the processing pipeline. The instrumental evaluation conﬁrmed the strong global impact of various arrays and rendering parameters on spectral balance and the overall level of rendered noise. They determined the audible threshold of coloring artifacts during head rotation for various array conﬁgurations in a perceptual user study. Helmholz et al. [] applied binaural rendering of a spherical microphone array signal to increase the SNR of the rendered signal by up to  dB with some array conﬁgurations with larger radii and spherical harmonic order four or higher microphones. Chazan et al. [] presented an integrated network for speech separation of an unknown number of speakers and presented a noise and reverberation dataset for Research on high-quality surround sound audio is based on a ﬁxed position of the recording microphone in general, such as in a movie theater. However, in Metaverse, users can change their listening position as they run, spin, or various body changes. Birnie et al. [] proposed a method for binaural playback of microphone recordings in a virtual application in which one’s body freely moves beyond the recording location. They integrate near, and far sources in an extended virtual environment and better reproduce the intensity and binaural room impulse response spectrum of the S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges Early room reﬂection estimation is an important task in audio signal processing, along with beamforming, source separation, room geometry inference, and spatial audio applications. Shlomo and Boaz [] proposed a solution for blind estimation of reﬂection amplitudes using iterative estimators based on maximum likelihood and alternating least squares. Blind estimation of direction of arrival (DOA) and delay of indoor reﬂections due to reverberation is useful for a wide range of applications, but conventional methods detect only a few reﬂections. Shlomo and Boaz [] proposed PHase ALigned CORrelation (PHALCOR) for estimating early reﬂex delay and DOA blinding. Head-Related Transfer Function (HRTF) is used to simu- late external sound by measuring the sound source’s spectrum in three-dimensional space. HRTF individualization enables realistic and immersive spatial audio rendering in Metaverse. Zhou et al. [] identiﬁed the lowest spectral distance error by exploring the range of HRTF predictability using a deep neural network with a 3D ear shape as input. In practice, binaural reproduction is also affected by HRTF, along with truncation errors that detrimentally affect the perception of the reproduced signal. Because pretreatment of HRTF by ear alignment prevents effective recognition, Ben-Hur et al. [] presented a method for integrating preprocessed ear-aligned HRTFs into the binaural regeneration process. HRTF is the key to audio spatialization. However, it cannot produce sufﬁcient sound output levels at low frequencies (below  Hz) while maintaining an omnidirectional pattern. To address this problem, Chojnacki et al. [] proposed a new design to overcome the limitations of this low-frequency range at higher frequencies. Gari et al. [] analyzed and rendered multi-channel RIR (Room Impulse Response) by parameterizing the sound ﬁeld as a series of plane waves for the Spatial Decomposition Method (SDM). They reduced the unnatural arrival direction diffusion of late reﬂections by spatial clustering of reﬂections in the post-processing and solved the whitening problem of late reverberations with a binaural RIR corrected equalization method, RTMod+AP. Ge et al. [] proposed DoodlerGAN, a generative part- based GAN (Generative Adversarial Network) that gener- ates creative and high-quality images to generate invisible conﬁgurations of new part shapes. They also introduced two creative sketch datasets: Creative Birds and Creative Creatures. Aiming to increase the resolution and level of detail within super-resolution images, Roziere et al. [] utilized an evolutionary method to improve NESRGAN+ by optimizing noise injection at proposed Diagonal CMA to optimize the injected noise according to a new criterion that combines quality assessment and realism. Lassner and Zollhofer [] proposed Pulsar, an efﬁcient sphere-based differential rendering module that is fast, modular, and easy to use. It avoids topological problems by using spheres for scene representation. It uses an efﬁcient differential projection operation and neural shading to alleviate topology inconsistency problems, high memory footprint, and slow rendering speed. The computational complexity of the transformer increases twofold with sequence length, making it impractical for many real-time applications. Wu et al. [] proposed an efﬁcient transformer-based acoustic model with constant speed regardless of input sequence length for streaming speech synthesis applications. They used the Emformer network to predict frame rate spectral characteristics in streaming and WaveRNN neural vocoder to generate the ﬁnal audio by taking the predicted spectral characteristics. They demonstrated consistent performance, low latency, and low real-time performance over various utterance lengths. Richard et al. [] presented a neural rendering approach for binaural sound synthesis that generates spatially accurate binaural sound in real-time. They proposed end-to-end neural binaural sound synthesis that outperforms DSP-based methods in a perceptual study and a qualitative evaluation. Control strategies for physically simulated characters per- forming two-person competitive sports (e.g., boxing and fencing) are used as a reference for effective motion rendering in the Metaverse. Won et al. [] developed a learning framework for generating control policies for physically simulated athletes with many degrees of freedom. They presented a control policy learned from a framework that generates both tactical and natural behavior. Ye et al. [] proposed a learning-based approach that infers an object’s 3D shape and poses from a single image and learns from segmented output of an off-the-shelf recognition system (i.e., shelf supervision). They inferred the volume representation of standard frames together with camera poses. After that, they performed shape-pose decomposition and instance-by- instance reconstruction of image collections in more detail. Yuan et al. [] proposed a STAR that performs self- supervised tracking and reconstruction of dynamic scenes with rigid motion in multi-view RGB video without manual annotation. By decomposing into two component parts and encoding each into its own unique neural expression simultaneously, the dynamic scene is reconstructed as a single solid object in motion. They also jointly optimized the parameters of the two neural luminosity ﬁelds and a set of ﬁxed poses that align the two ﬁelds in each frame. Ng et al. [] studied body motions for 3D hand shape synthesis and estimation in the area of conversational gestures based on the assumption that body movements and hand gestures are strongly correlated in non-verbal communication environments. Hand prediction model generates a 3D hand gesture with only the 3D motion of the speaker’s arm as input. Eisenberger et al. [] proposed a neural network architecture NeuroMorph that takes two 3D shapes as input and generates them at once in an end-to-end learning S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges method. It is in a fully unsupervised manner without manual correspondence annotation. By combining graph convolution with global feature pooling to extract local features, geodesic lines are approximated in this shape-space manifold to produce realistic deformations. a: MULTIMODAL CONTENT REPRESENTATION In the task of retrieving linked query images from a database, Chen et al. [] proposed to express an image as a constituent object based on the intuition that the ﬁnest detail of manipulation is often at the object level. They introduced an object-embedding framework for OE-SIR (Spliced Image Retrieval) using object detectors and a teacher-student model Kiela et al. [] introduced Dynabench, an open-source platform that runs in a web browser and supports the creation of human-in-loop model datasets for dynamic model bench- marking. With Dynabench, data set creation, model devel- opment, and model evaluation inform each other directly, making it a more powerful and informative benchmark. Current models for Word Sense Disambiguation (WSD) are human-level performance in global WSD metrics but lack data to model and evaluate rare senses. Blevins et al. [] established criteria for FEWS using knowledge-based neural WSD approaches and better captured rare sensations in the WSD dataset with a model further trained with FEWS. c: MULTIMODAL ENTITY LINKING AND EXPANSION Context and entity afﬁnity are mainly captured via vector dot products, potentially missing ﬁne-grained interactions between them, requiring large memory footprints to store dense representations. De Cao et al. [] proposed GENRE to generate unique names for each token in a left-to-right autoregression method and search for entities according to context. It directly captures the relationship between context and entity name, effectively cross encoding both and greatly reducing memory footprint because it scales with the lexical size rather than the number of entities. For the motion transfer task between the one dancer and the target person, Gafni et al. [] proposed a model to re- animate a single image with an arbitrary video sequence. They combine three networks: a segmentation mapping network, a realistic frame-rendering network, and a face Data augmentation methods experience distribution shifts and consequently degrade the performance of non-augmented data during inference. Gong et al. [] used a saliency map to detect important regions in the original image and preserved these information regions while augmenting them. Because moments extracted from instance normalization and position normalization roughly capture the style and shape information of the image, Li et al. [] proposed Moment Exchange, an implicit data augmentation method that encourages models to utilize moment information in recognition models as well. Knowledge Distillation (KD) tends to make inconsistent predictions when the data distribution changes slightly, so a method is needed to apply it to low-resource (both memory and computational) platforms. Liang et al. [] proposed MixKD, a data- agnostic distillation framework that utilizes a simple but efﬁcient data augmentation approach to give the resulting model stronger generalization capabilities. Jia et al. [] systematically studied whether the extent visual information (i.e., objects and contexts) contributes to understanding human motives to analyze how visual information easily recognizes human intentions behind social media images. They introduce Intentonomy, an intent dataset consisting of 14K images covering a wide range of everyday scenes to study the present intentions. When training intent classiﬁers, they performed additional studies to quantify the effects of attending object and context classes and textual information in the form of hashtags. Huang et al. [] implemented a post-processing step with simple modiﬁcations to the standard label propagation technique in the initial graph-based semi-supervised learning method. A cyber-physical digital of a non-software (physical) system, which has recently received much attention, but its cyber-cyber response is relatively overlooked. Ahlgren et al. [] measured the practical impact on digital twins’ design, implementation, and deployment as conceptually true twins by simulating Speech recognition-based natural language dialog is the basic medium of user interaction. Recently, BlenderBot . [] was proposed based on two studies: engine-based generation and long-term memory integration, as shown in Fig. . The LM-based dialog generation model has the hallucination problem of generating plausible sentences that are factual. To prevent this problem, searching using the Internet and generating a ﬁnal response based on the searched information was proposed. In addition, in most conversational studies, many short conversations (typically - turns) consist of a single conversation session because the dialogue engine gives scenario-speciﬁc answers rather than responses based on long-term memory. The proposed model provides improved search capabilities with the ability to summarize and recall previous conversations. However, since it is a model with an open dialogue that can expose personal behavior (e.g., long-term memories and the S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges FIGURE . Facebook Chabot Blender bot ., which utilize memorization and internet, search speaker’s personal interest), careful attention is required in Reference games illustrate the functional use of language for communication and provide a basic learning environment for neural agents. Languages are inherently biased by the underlying capabilities of agents. Dagan et al. [] introduced the Language Transmission Simulator to model agent populations’ cultural and architectural evolution. They emphasize the importance of studying basic agent architec- tures and propose coevolution of languages and agents in the study of language emergence. With the recent development is widely used for various tasks of natural language and various modals. Recurrent Neural Network Transducer (RNN-T) is a famous method in automatic speech recognition due to its simplicity, conciseness, and general transcription, but it lacks an external language model and is more vulnerable to rare long-tail words (e.g., entity names). Le et al. [] proposed RNN-T to model intractable rare WordPieces by injecting additional information into the encoder and using alternative letter pronunciations. Deep fusion with personalized language models for stronger biasing. Weber et al. [] considered language modeling as a multi-task problem, combining three studies: multi- task learning, linguistics, and interpretability, to analyze the generalization behavior of language models in Negative QA is the most basic solution for communicating with NPCs in the Metaworld. Annotated data sets are difﬁcult and expensive to collect and rarely exist in languages other than English. That is the reason it is hard to build a QA system that works well in other languages. Lewis et al. [] proposed a multi-dimensionally ordered extractive QA evaluation benchmark MLQA. Xiong et al. [] proposed a simple and efﬁcient multi-hop dense search approach to answer complex open-domain questions, achieving state-of-the-art performance in two multi-hop data sets, HotpotQA and multi- evidence FEVER. Min et al. [] proposed a model to FIGURE . Multimodal training with videotext representation build a system that can predict correct answers in open QA that receives natural language questions as input and returns natural language answers while meeting strict disk memory budgets. Memory budgets encourage agents to explore a balance between storing parameters for large and redundant search corpora and large training models. ral interface while covering a wide range of Metaverse. Because the common language (e.g., English) has limita- tions for ﬂuent communication, multilingual translation is required to provide a natural interface in other languages. Schwenk et al. [] presented a multilingual sentence embedding-based approach to automatically extract parallel sentences from the content of Wikipedia articles in  lan- guages. Other modalities tend to generate similar decoder representations and preserve more information in pre-trained translation modules. Tang et al. [] proposed a parameter sharing and initialization strategy to enhance information sharing between tasks. It is a new attention-based regularization for encoders and an online knowledge distil- lation method to improve knowledge transfer. The quality assessment aims to measure the quality of translated content without access to reference translations. Tuan et al. [] proposed a method that does not S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges Noise contrast learning for videotext representation learn- ing increases the similarity of representations of pairs of known samples and repels all other representations. Patrick et al. [] proposed a method to mitigate this by using generative models to push these related samples naturally, as depicted in Fig. . The captions of each sample were reconstructed as weighted combinations of the visual representations of other supporting samples. It is difﬁcult to learn the grounding of each word due to noise and the presence of words that cannot be visually meaningfully grounded. Meng et al. [] presented a jointly trained model architecture for controlled trace generation and controlled caption generation. They proposed a local bipartite matching (LBM) distance measurement that compares two traces of different lengths to evaluate the quality of the generated trace. Because audio and video signals are not always informed of each other, audiovisual correspondences often result in false positives. It optimizes the weighted- contrast learning loss and lowers its contribution to the overall loss. Morgado et al. [] optimized the instance identiﬁcation loss with a soft target distribution that estimates the relationship between instances. Morgado et al. [] optimized visual similarity rather than simple cross-modal similarity using SS based on contrast learning with cross- modal audio and visual recognition. Szot et al. [] proposed a simulation platform for training virtual robots in interactive 3D environments and com- plex physics-based scenarios. Experimental results showed that ﬂat RL policy suffers from HAB (Home Assistant Benchmark) compared to the hierarchical policy, hierarchical structure with independent technology suffers from takeover problem. In audiovisual exploration, agents use both sight and sound to move through complex and unmapped 3D environments intelligently. Chen et al. [] showed how to operate at a ﬁxed granularity of agent behavior and rely on simple iterative aggregation of audio observations, as shown in Fig. . It uses waypoints that are dynamically set, and end-to-end learned within the search policy. Acoustic memory provides a structured and spatially based record the agent hears as it moves. Recent work on audiovisual navigation assumes a continuously audible target, and the role of audio in announcing the target’s location is limited. Chen et al. [] introduced semantic audiovisual exploration in which objects in the environment make sounds consistent with their semantic meaning (e.g., ﬂushing toilets, creaking doors) and in which acoustic events are sporadic or short-duration. They proposed a converter-based model for handling this new semantic AudioGoal task by incorporating an inferred goal descriptor that captures an object’s spatial and semantic properties. Persistent multimodal memory allows the target to be reached even after the acoustic event has stopped. ObjectGoal Navigation (OBJECTNAV) is the task of an agent navigating object instances in an invisible environment, which degrades performance due to overﬁtting and sample inefﬁciency. Ye et al. [] integrated the learned components and motivated methods that operated on explicit spatial maps of the environment and reactivated the general learning agent by adding auxiliary tasks and navigation It’s unclear how to optimize the layout of 3D UI controls for body and aerial interactions. Li et al. [] evaluated the performance and limitations of a non-dominant ﬁxed 3D UI in a VR environment through a two-handed pointing study. It has been demonstrated that targets that appear closer to the skin (i.e., located around the wrist placed on the inside of the forearm) can be selected faster than targets that are further away from the skin (i.e., around the elbow on the side of the arm). Bagautdinov et al. [] presented a learning-based method for constructing a driving signal recognition whole body avatar. They generate high-quality representations of human geometry and view-dependent shapes using conditionally deformable auto-encoders that are animated with imperfect driving signals (e.g., human poses and face key points). Better drivability and generalization were achieved by separating the unusable driving signals and the rest of the generated elements during animation. Modeling thin structures (e.g., hair) has low resolution and is too slow. Lombardi et al. [] showed a dynamic rendering representation that combines the completeness of a physical representation with the efﬁciency of primitive-based rendering. It utilizes spatially shared computations with a convolutional architecture and uses volumetric primitives that are moved to cover only the occupied portion of space. Sun et al. [] introduced a hair inverse rendering framework for reconstructing high-ﬁdelity 3D geometry and reﬂectivity of hair that is easily used for realistic rendering of hair. They proposed a new solution for line-based multi-view stereo that calculates accurate hair geometry from multi-view metering data and estimate hair reﬂection characteristics using multi-view metering data. In the Metaverse, avatar clothing is not just for decoration but a means of providing immersion and emphasizing social roles. To create high-deﬁnition animations, Xiang et al. [] proposed a method to create an animable clothed body avatar by explicitly representing the upper body’s clothing in a multi-view capture video, as shown in Fig. . To separately register 3D scans with the template using a two-layer mesh representation and to improve photometric responsiveness, they perform texture alignment through the inverse rendering of the garment geometry and texture predicted by the deformation autoencoder. Chaudhuri et al. [] proposed ReAVAE (Region-adaptive Adversarial Variational Varia- tional AutoEncoder) that learns the probability distribution of each region individually to generate various high-ﬁdelity texture maps for 3D human meshes by sampling from the distribution for each region. They present a data generation S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges FIGURE . Learning to set waypoints for audio-visual navigation in the indoor environment []. weakly supervised manner from low-topology mesh tracking on training images. It is strong at testing expressions and opinions about people of different genders and skin colors. Self-supervised pre-training can outperform full-supervised training and is useful in preventing overﬁtting to smaller data sets. Shukla et al. [] showed the potential of visual self-supervision for learning audio functions. They proposed that joint visual and audio self-supervision leads to more informative audio representations for speech and emotion recognition. The proposed multi-task combination of visual and auditory self-supervision is useful for learning more powerful and rich functions in noisy conditions. Procedurally generated environments require algorithmically generated environment instances using a unique variable factor conﬁguration as an important benchmark for testing systematic generalization in deep reinforcement learning. Jiang et al. [] proposed Prioritized Level Replay (PLR), a general framework for selectively sampling the next level of training by prioritizing items that are expected to have higher learning potential upon future revisit. TD errors lead to new curricula of increasingly difﬁcult levels when used to effectively estimate the future learning potential of a level and guide the sampling procedure. Modhe et al. [] proposed a novel framework that provides exploration and sample complexity to identify sub-objectives that are useful for exploration in sequential decision-making tasks under partial observability. They utilized a variant-speciﬁc control framework that maximizes empowerment to reach various states reliably. It identiﬁes sub-goals as states with high essential, optional information through the normalization of high-dimensional sensory observations, learning controllable embeddings (LCEs) embed observations in low-dimensional latent space and estimate latent dynamics to perform control in latent space. Cui et al. [] proposed a modiﬁed value- guided CARL that optimizes the weighted version of the FIGURE . The process of cloth rendering which includes single-layer surface tracking and inner-layer shape estimation []. FIGURE . Face rendering with per-object decoding and per-pixel decoding in Pixel Codec Avatar []. technique that augments the training set with data taken from It can be generalized to natural lighting conditions, but it is computationally expensive to render. Bi et al. [] presented a method to build animable high-deﬁnition 3D face models that can pose and render in real-time in a novel lighting environment. They train a generalizable model and use it to generate a training set of high-quality synthetic face images under natural lighting conditions. The neural shading phase accounts for deformations that are not captured in the mesh and alignment inaccuracies and dynamics that confound the DNR pipeline. Raj et al. [] proposed Articulated Neural Rendering (ANR), a DNR- based framework that explicitly addresses the limitations of virtual human avatars. Ma et al. [] proposed Pixel Codec Avatars (PiCA), a deep generation model of the 3D human face that is computationally efﬁcient and adaptable to in-run rendering conditions while achieving state-of-the- art reconstruction performance as depicted in Fig. . They use a fully convolutional architecture for decoding spatially varying features and a rendering adaptive per-pixel decoder to integrate through dense surface representations learned in a S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges CARL loss function whose weights depend on the TD- error of the current policy. In the ofﬂine implementation, the local linear control algorithm (e.g., iLQR) used in the existing LCE method was replaced by the RL algorithm (i.e., a model-based soft actor-critic). Model-based reinforcement learning is a method that utilizes control-based domain knowledge to improve the sample efﬁciency of reinforcement learning agents. Policies tend to lag behind model-free agents in terms of ﬁnal rewards, especially in environments where they are not critical. Amos et al. [] found an effective combination of model-free soft value estimation for policy evaluation and model-based stochastic value gradient for policy improvement for model-based high-dimensional Sukhbaatar et al. [] proposed Expire-Span, which learns how to retain the most important information and expire information, as not all past contents need to be remembered equally. To evaluate models for lifelong learning tasks, Abdelsalam et al. [] developed a stan- dardized benchmark that enables model evaluation in IIRC settings. Methods incorporating network scaling naturally add model capacity for learning new tasks while avoiding catastrophic oblivion, but increasing the number of additional parameters is computationally expensive at larger scales. Verma et al. [] proposed a simple task-speciﬁc feature map transformation strategy for continuous learning called Efﬁcient Feature Transformations (EFT). It adds a minimal number of parameters to the underlying architecture, pro- viding strong ﬂexibility to learn new tasks. To solve the catastrophic forgetting problem in a sequential task where data from previous tasks are not available, Mehta et al. [] proposed a principled Bayesian nonparametric approach, the Indian Buffet Process (IBP), which determines how much the data scales to model complexity. The IBP dictionary promotes positive knowledge transfer between tasks by encouraging sparse weighted element selection and element reuse. The goal of continuous learning (CL) is to learn a series of tasks without experiencing catastrophic forgetting. Ebrahimi et al. [] proposed a simple educa- tional paradigm, Remembering for Right Reasons (RRR), by encouraging explanations so that models have the right The beneﬁt of multi-task learning is that it uses relationships across tasks to improve the performance of a single task. Metadata is useful for improving multi-task learning perfor- mance, but effective integration is an additional challenge. Sodhani et al. [] showed state-of-the-art results in Meta- World, which consists of a challenging multitasking bench- mark. It learns expressions that are interpreted as metadata construct and how to construct them. Fu et al. [] proposed a framework LeTS that utilizes multi-task computation and parameter sharing for efﬁcient ﬁne-tuning. It decouples the computational dependencies of existing ﬁne-tuning models with a neural architecture that reuses intermediate results and reduces computational demands by leveraging the sparsity feature of weight differences. Zhang et al. [] proposed Hidden-Parameter Markov Decision Processes (HiP-MDPs), an explicit modeling method for this structure, to improve sample efﬁciency in multitasking settings. In the HiP-MDP setting, they utilized the idea of a common structure and extended to enable state abstraction inspired by block MDP. Dollar et al. [] proposed a simple and fast complex scaling strategy that scales the underlying convolutional network to give greater computational complexity and, con- sequently, expressive power, extending the model. It provides a framework for analyzing scaling strategies under various computational constraints. Ruiz and Verbeek [] proposed Hierarchical Neural Ensembles (HNE) to handle scenarios where the amount of computation and input data varies with time. It includes an ensemble of multiple networks in a hierarchical tree structure that shares an intermediate layer. As a hierarchical distillation to increase the prediction accuracy of small ensembles, the overlapping structure of the ensembles is utilized to allocate accuracy and diversity across GPU performance and efﬁciency of recommendation models are affected by model architecture conﬁgurations (e.g., dense and sparse features and MLP dimensions). Acun et al. [] described the complexity of using GPUs for training recom- mendation models, factors inﬂuencing hardware efﬁciency at scale, and a new scale-up GPU server design from Zion. Silent Data Corruption (SDC) is a negative impact on large- scale infrastructure services. Dixit et al. [] provided a debug ﬂow based on the root cause and classiﬁcation error guidance within the CPU using case studies as an explanation of how to debug this class of errors. Vanilla NAS provides real-world performance, as each architecture is evaluated through training from scratch, but it is time-consuming. Zhao et al. [] showed that one-shot NAS signiﬁcantly reduces the computational cost by training only one supernetwork to approximate the performance of all architectures in the search space through weight sharing. To mitigate unwanted joint adaptation, they proposed several NAS using multiple supernetworks, called sub-supernets, each covering different areas of the search space. Stage  NAS needs to sample from the search space during training, which directly affects the accuracy of the ﬁnal searched model. Uniform sampling has been widely used for simplicity is agnostic to the model performance Pareto front, which is the primary focus of the search process, missing the opportunity to improve model accuracy further. Wang et al. [] proposed an AttentiveNAS that focuses on enhancing the sampling strategy. MBRL algorithms are complex due to the separate dynamic modeling and follow-up planning algorithms. S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges Consequently, when possessing dozens of hyperparameters and architectural choices, signiﬁcant human expertise is required before applying them to new problems and domains. Zhang et al. [] used automatic hyperparameter optimiza- tion (HPO) to improve performance compared to using static hyperparameters ﬁxed for the entire training during training itself. Zhang et al. [] studied how expressive learning can accelerate reinforcement learning from rich observations (e.g., images) without relying on domain knowledge. The bi-simulation metric quantiﬁes the behavioral similarity between states in continuous MDP and trains the encoder so that the distance in the latent space is equal to the bi- simulation distance in the state space. For robust, fast, and scalable binary optimization, Panchenko et al. [] proposed Lightning BOLT, an improved version of the BOLT binary optimizer that signiﬁcantly reduces the processing time and memory requirements while maintaining the efﬁciency of the BOLT, which enhances the performance of the ﬁnal binary. Empirical risk minimization (ERM) is generally designed to perform well for mean loss so that sensitive to outliers, does not generalize, and treats subgroups unfairly. Li et al. [] explored the problem through an integrated framework called TERM (Tilted Empirical Risk Minimization) to increase or decrease the impact of outliers, respectively. They show that TERM is used in a variety of applications (e.g., enhancing fairness between subgroups, mitigating the effect of outliers, and handling Fairness and robustness are two important concerns of federated learning systems. Robustness to data and model poisoning attack and fairness are the constraint to compete in statistically heterogeneous networks, as measured by the uniformity of performance across devices. Li et al. [] proposed to use Ditto, a simple and general framework for personalized federated learning and developed an extensible solver for this. To understand and improve fault tolerance training for deep learning recommendations with partial recovery, Maeng et al. [] optimized CPR, a partial recovery training system for a recommendation model. It relaxes consistency requirements and improves failure- Unexpected reboots disrupt services running on the hard- ware and reduce ﬂeet availability. A server reboot is also an important signal that indicates an underlying problem (e.g., a memory leak in service, a catastrophic hardware failure, a power outage) in a data center. Lin et al. [] provided a large-scale, near-real-time reboot-monitoring framework that supports machine learning-based anomaly detection and automated root cause analysis for hundreds of server attribute combinations. Xia et al. [] proposed Facebook’s risk-focused backbone management strategy to ensure high service performance during the COVID- pandemic. It has been shown to achieve high service availability and low path scalability while resiliently withstand stress tests and handles Oughton et al. [] anticipated that 5G would remain the preferred technology for wide-area coverage, while Wi-Fi  will remain the preferred technology indoors thanks to its much lower deployment costs. To address the problem of packet loss affecting a wide range of applications using Voice over IP (VoIP), Lin et al. [] proposed prediction and mask training to improve the performance of the CRN framework. It outperforms the reference system using only the LSTM layer in terms of two objective metrics: speech quality (PESQ) and short-term objective intelligibility (STOI). The CRN consists of a convolutional encoder- decoder structure and an LSTM (long short-term memory) is suitable for real-time speech enhancement Applying homogeneous encryption (HE) to the client- cloud model allows the cloud service to perform inference directly on the client’s encrypted data. However, HE satisﬁes privacy constraints, but it introduces enormous computational problems in the current system. Reagen et al. introduced Cheetah, a set of algorithms and hardware optimizations for server-side HE DNN inference to approach real-time speed. Automatic compilation of an efﬁcient HE kernel in a synthetic compiler for vectorized isomorphic encryption is relatively unexplored. Cowan et al. [] proposed an optimizing compiler, Porcupine, which uses program synthesis to generate vectorized HE code. Porcupine captures the underlying HE operator behavior and automat- ically infers the complex trade-offs imposed by these issues to develop an optimized and validated HE kernel. Carrying suspended payloads is difﬁcult for autonomous aircraft, and rapid in-ﬂight adaptation to payloads with physical properties unknown a priori remains an open question. Belkhale et al. [] proposed a meta-learning approach that learns to learn a modiﬁed dynamics model within seconds of ﬂight data after connection. One way to infer the safety of a robot is to build a safe set through Hamilton-J, but because of the long computation time, it sometimes assumes perfect knowledge of the mechanics, and the safety set is calculated ofﬂine. Shih et al. [] proposed a new framework for learning safety control policies from simulation and using it to generate online safety sets from uncertain dynamics. As climate change increases the frequency and severity of natural disasters, response organizations need improved data to better understand the dynamics of disaster impacts. Giraudy et al. [] are leveraging Facebook Location History (LH) data as part of its disaster mapping initiative to enable location-based services (e.g., Nearby Friends, location-based advertising) and social value products (e.g., disaster maps) to help people S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges Diplomacy is a game of switching alliances that involves both cooperation and competition, which is not successful in large-scale games involving collaboration. Gray et al. [] described a media-free Diplomacy transforming agent that combines supervised learning on human data with one-step preview search through minimizing external regrets. Ha-Thuc et al. [] discussed how these systems evolve from traditional formulations by incorporating producer values into goals. Jointly optimizing the ranking function for both consumer and producer value is a new direction and raises many technical challenges. They make the layout an end-to-end solution and describe the results of applying it to Facebook Marketplace. Blackshear et al. [] proposed a method for blockchain asset owners to recover their funds if their private key is accidentally lost or sent to the wrong address. They achieve this with a Commit, Reveal, Claim, and Challenge smart contract that allows access to funds at addresses where the spend key is unavailable. The auction market introduces the concept of speed balance by reinterpreting the process of applying a coefﬁcient between  and , which equalizes bids in all auctions on behalf of each buyer. Conitzer et al. [] showed that calculating the social welfare maximization and proﬁt maximization rate equilibrium is NP-hard but presents a mixed-integer program (MIP) is used to ﬁnd a balance that optimizes several related goals. It uses static MIP solutions to improve the results achieved with dynamic pacing algorithms using instances Online social network (OSN) accounts exhibit many demo- graphic attributes (e.g., age, gender, location, and occupa- tion). Onaolapo et al. [] devised a method to instrument and monitor stolen social accounts to understand the impact of demographic characteristics on attacker behavior. Cyber- criminals accessing teen accounts create more messages and posts than cybercriminals accessing adult accounts, while attackers compromising male accounts destroy, including changing some of their proﬁle information. Cybercriminals accessing female accounts appeared to be engaged in hostile activity. Bailey et al. [] explored the spatial structure of social networks in the New York metropolitan area, where a signiﬁcant proportion of city dweller connections are with nearby individuals. By examining the importance of transport infrastructure, they document signiﬁcant heterogeneity in the geographic extent of social networks and show that this heterogeneity is correlated with public transport use. In the present state of sharing both temporary and permanent content on social media platforms, Luria and Foulds [] discussed our ﬁndings on the short-term and long-term transitivity as part of social media experiences and the evolving identities of teens and young adults. As long as proportionality is not violated, there are greedy algorithms that involve volunteers and non-adaptive methods that include volunteers with trait-only probabilities assuming that the distribution of common traits in the volunteer pool known. Although this distribution is not known a priori, Do et al. [] proposed a reinforcement learning-based Fernanda [] proposed the knowledge framework by using a mix of quantitative and qualitative methods to explore the current state of diversity and representation in online advertising and people’s attitudes to the impact of diversity on digital campaign performance. More frequent and positive portrayals of underrepresented and diverse groups have a signiﬁcant positive impact on business outcomes. It is important to optimize advertisers’ budgets for campaigns across platforms without knowing the value of serving ads to users on multiple platforms. Avadhanula et al. [] provided a regret algorithm for individual bid spaces. The generalization of existing MAB algorithms (e.g., Upper Conﬁdence Bound and Thompson Sampling) does not perform well in two applications: the intelligent SMS routing problem and the advertising audience optimization problem that many businesses (especially online platforms) face. Sinha et al. [] presented a simple variant of explore-the- commit and improved performance by setting a near-optimal regret range for this algorithm. Because simulation provides the ability to train a large robots in parallel and provides rich data, Truong et al. [] used educational simulations before deploying the robots. They proposed bidirectional domain adaptation (BDA), an approach that connects the sim-vs-real gap in both directions for point goal navigation. They use Real2sim for bridging the visual domain gap and sim2real for linking the dynamic domain gap. ) DISCUSSION AND OPEN CHALLENGES As mentioned above, Facebook research is a research group with a lot of interest in Metaverse, as shown in Table . It has broad elemental technologies for natural language, vision, dialogue, and embodiment. It also has a foundation and experience that is expanded into a Metaverse platform with a Facebook social network service. Essential models for Metaverse are Blenderbot [] based on PariAI, Detectron  [] capable of fast visual recognition, and Habitat [] that operate an agent from an eco-centric point of view. They provide services by launching its own Metaverse platform, Horizon and Inﬁnite Ofﬁce. The virtual currency Dime not only serves as a bridge between reality and the Metaverse but also leads to a sustainable ecosystem. VI. DISCUSSION AND OPEN CHALLENGES In this section, we discuss current problems and technologies needed in the future for Metaverse in the aspect of inﬂuence, limitations, and open challenges. S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges TABLE . Use case - Facebook Metaverse. A. METAVERSE INFLUENCE FOR USER AND SOCIETY ) SENTIMENT AND SOCIAL INFLUENCE People can lead a stable cyber life in the Metaverse because they can distinguish between real-life and virtual life, just as a person did not feel confused while watching an avatar movie. However, because avatar design has emotional barriers, users may feel a sense of rejection towards the avatar if it cannot overcome Uncanny Valley like in the Alita movie. S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges Memories are beautiful because they are exquisitely crafted memories but because they are traces of time that cannot be returned. However, Metaverse recreates the past, and users can make different choices, giving people psychological stability and emotional recovery. While the social impact depends on the ecosystem, it is important to consider many aspects of social impact, includ- ing potential exacerbation of social inequities, computation demands, economics, legality, and ethics. In addition, limited resources in the real world bring excessive competition and social side effects. However, in the Metaverse, it has an advantage over the real-world system in terms of item resources that can be created indeﬁnitely online rather than a deduction compensation from limited resources in the real- life world. This is different from the reward system in the real world, where you must give up the other to get one, so it can reduce competition between users and is an opportunity to ) USER PARTICIPATION AND BENEFIT In the Metaverse, users are less limited by time and space and can exist in multiple places through avatars, so the communication style is changed from :N broadcasting to : interaction. Avatar in the Metaverse provides a way to replace and complement users. The Metaverse is most effective in places like Africa where experiential education is difﬁcult (e.g., undeveloped areas). In addition, High-contrast vision, long-distance vision, and volume augmentation for people with visual difﬁculties enable people with disabilities to live the same lives as ordinary people in Metaverse. Mask effects (i.e., hide shapes, colors, and races) are also noteworthy, providing a better-than-realistic user engagement experience. Origin, gender, skin color, and appearance can be prejudicial when it comes to debates, psychi- atric group therapy, and jury attendance in court. In this case, the avatar’s neutral appearance is a good example of the Metaverse’s social inﬂuence, which allows for a fairer opinion and participation in social consensus without In order to maintain a sustainable social ecosystem, user participation is important, so it fashion, games, and events on a regular and long-term basis. For example, in ZEPETO, users take selﬁes, solve quizzes, create dramas, and have fun designing costumes. In Meta- verse sports and gun games, it is possible to induce and increase user participation by providing a third-person view rather than a ﬁrst-person spectator mode. Metaverse can signiﬁcantly contribute to a multitude of appli- cations and domains. However, for sustainable Metaverse function, and ethics. More Metaverse applications help people work smart (e.g., layer separation and tagging for complex organ surgery, commuting to work simulation, remote problem identiﬁ- cation, mapping to real environments without the need to ﬁnd manuals). Metaverse makes living easy (e.g., senior public transportation simulation, intuitive simpliﬁcation of digital input interfaces for the elderly, immersive education more effective than a video, counseling personal with masked avatar). Metaverse applications reduce physical object and space (e.g., non-shared private messaging in the desired place, providing information through overlay display of ofﬂine objects, virtual screen, store inventory trends, sales volume display, virtual display for IoT devices, and smart Many advantages and applications have been described, but the sustainability of Metaverse is important. When the world’s population is maintained at a certain level, it can grow and ﬁx problems, but when the number of users accessing decreases, the world cannot be maintained. In the concept of life logging, the sustainability of various social relationships is more important than each event and task (e.g., games and simulations). In order to maintain continuity, a connection relationship (e.g., Metaverse access, messenger) must be maintained continuously in a relatively low-spec mobile device that can always be accessed. Using an episodic memory that effectively manages the user’s log allows the user to feel the comfort and advantage of accessing Metaverse for a long time. Storing all experiences in memory storage has limitations in utilization and capacity, so memory research on effectively ﬁnding and reusing important episodes is needed. In addition, latecomer platforms should consider import/export methods that bring the existing user experience and provide continual usability. ) HARDWARE AND SOFTWARE LIMITATIONS In terms of a sensor in hardware, while the Metaverse resembles the real world a lot, some sensations are better felt in real life (e.g., day sunlight, smell, stickiness, slippery, wind). In terms of software, programs developed in the Metaverse without coding are used as a basis for high compatibility in the Metaverse world. However, as the program becomes more sophisticated, it faces the limit of sophistication in a complex application. the dialog is developing into a longer and more natural form of conversation based on persona, but it is still limited as a sustainable lifelong- learning conversation solution with various perspectives and philosophies beyond exciting conversations. Humans basi- cally have multi-personas, and they are expressed differently depending on time and place. Therefore, it is necessary to study more complex persona modeling in consideration of the situation. From this point of view, environments and events are important to show the various personas of users and NPCs. For example, in the drama Westworld, avatars S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges perform various actions in the Metaverse, freeing from the constraints and conditions of reality. Therefore, NPCs in the role of residents of the Metaverse must be able to cope with various unexpected situations because the allowable range of scenarios is wide. In addition, the persona’s design is important for the NPC to appear as if they choose with their persona and will. NPCs can be in the form of humans and various living (e.g., horse, dog, cat) and non-living forms In Metaverse, since it is a comprehensive solution in which various tasks occur simultaneously in a complex form (e.g., multi-mode and multi-task), there is a lot of work to study for individuals to start development without experience. From the perspective of Metaverse development, there are few online resources to learn, especially for novice developers. There is not enough information for practical details to make complex and realistic implementations (e.g., object selection, conditional actions, user storyboards with scene ﬂow, teleportation between scenes, movement, and dialogue). For this reason, a collaborative system (i.e., a platform and developer community) for an individual developer is important to co-develop without designing the entire system. As for the platform, a commercial platform (e.g., Roblox) with favorable maintenance and an open source-based plat- form (e.g., Unity) with various possibilities are considered. Since the scope of the technology target of Metaverse is wide, it is necessary for the developer community to separate threads based on well-organized taxonomy and maintain a group of experts who lead in each technical AR uses lightweight devices, suitable for short experiences, but VR relatively needs heavy and expensive devices for long experiences. Some approaches switch between AR and VR in one piece of hardware by mixing the advantages of AR and VR. Although this method has the advantage of using AR and VR in alternative ways, it becomes expensive and heavy compared to a single model device. Alternatively, holograms are not a popular technology in Metaverse, but they have Eye-worn lenses are another input method utilized in the Metaverse (e.g., Maya Lenz, Mirage, Mojo lenz). The lens analyzes the user’s information by tracking the direction of eye movements, focus, blinks, and winks. For example, Maya Lenz is a wearable device in the form of a contact lens, and Mirage is a way of expressing disliked content by replacing it with positive alternatives. Mojo lenz is used in conjunction with an assistive device worn around the neck to seamlessly process a variety of visual information (e.g., data feeds, people’s proﬁles, video calls, translations, notiﬁcations) into Privacy and security are critical issues because Metaverse collects data on behavior that is more detailed than user conversations and internet history. Avatar two-factor authen- tication and protection of transmitted data are essential, and we need to be more vigilant with regard to crimes that may occur on the Metaverse. In addition, surveillance actions and follow-up review) due to the surge in users suggest that organizations that play the same role as police and government are needed in the real world. There are some instances where exemplary people in the real world commit crimes based on their online anonymity in the Metaverse. The norms and restrictions of the Metaverse may differ from those in the real world because they have a post-nationalism and degrees of freedom. Most users familiar with the Metaverse are the young generation with relatively various social ideas. It is necessary to build a Metaverse with a worldview and ethical consciousness in which various avatars can live, rather than a Metaverse as a physical space. ) INTERDISCIPLINARY RESEARCHES Since the Metaverse consists of a world that changes in real-time for a large number of users and NPCs, cross- disciplinary research is necessary. As an example of cross- disciplinary research, Metaverse leverages knowledge widely used in cognitive science (e.g., episodic memory, intrinsic motivation, and theory of mind) to provide more immersive and sustainable services. Episodic memory occurred a long time ago in the present conversation and induced a natural conversation. Intrinsic motivation allows an agent to perform multiple tasks rather than a single task consistently. The theory of mind has the advantage of deepening conversation to understand from the other person’s point of view. Other examples are the social sciences, psychology, and economics. The environment in which a certain number of members live using masked avatars differs from how society currently operates. Neuroscience and psychological approaches for psychotherapy are used to understand humans and maintain a Metaverse deeply. The virtual currency of Metaverse is different from the virtual currency in the real environment, so it can become a new variable from the point of view of economics and develop into a fused form. In this study, we analyzed research for similar concepts of Metaverse in Metaverse, avatar, and XR. After that, we com- prehensively dealt with the necessary three components (i.e., hardware, software, and contents) for Metaverse. We also reviewed the latest trends of Metaverse approaches (i.e., user interaction, implementation, and application) that were currently available and necessary in the future. Interacting as part of the story is important rather than seeing well- formed storytelling and immersive visual effects. We applied S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges taxonomy to three famous Metaverse domains (i.e., movie, Facebook Research. Finally, we discussed the aspect of social inﬂuence, limitation, and open challenges. From a future-oriented perspective, Facebook research tries to input text using the output of the peripheral nervous system and brain-computer interface. As a direct connection method, Neuralink is a way to enhance communication with devices by implanting a chip in the human brain. The current TABLE . List of main acronyms. stage of development is to the extent that it is possible to directly stimulate a speciﬁc part of the brain and look at a simple type of EEG. However, the continuous development of brain-computer-interface and Neuralink can develop into a form that gives an experience that is difﬁcult to distinguish from reality in the Metaverse (e.g., the method of connecting S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges [] Roblox Blog, Roblox Corp., . Accessed: Nov. , . [Online]. Available: https://blog.roblox.com/ [] C. Meier, J. Saorín, A. B. de León, and A. G. Cobos, ‘‘Using the Roblox Video Game Engine for Creating Virtual tours and Learning about the Sculptural Heritage,’’ Int. J. Emerg. Technol. Learn., vol. , pp. –, [] R. U. Long, ‘‘Roblox and effect on education,’’ M.S. thesis, Dept. Educ., Drury Univ., Springﬁeld, MO, USA, . [] J. Kemp and D. Livingstone, ‘‘Putting a second life ‘metaverse’ skin on learning management systems,’’ in Proc. 2nd Life Educ. Workshop Second Life Community Conv., San Francisco, CA, USA, vol. , , pp. – [] A. M. Kaplan and M. Haenlein, ‘‘The fairyland of second life: Virtual social worlds and how to use them,’’ Bus. Horizons, vol. , no. , [] M. R. Cagnina and M. Poian, ‘‘How to compete in the metaverse: The business models in second life,’’ U Udine Econ. Work. Paper, Udine, Italy, Tech. Rep. -, , doi: . [] H. Duan, J. Li, S. Fan, Z. Lin, X. Wu, and W. Cai, ‘‘Metaverse for social good: A university campus prototype,’’ in Proc. 29th ACM Int. Conf. Multimedia, Oct.  pp. –. [] H.-S. Choi and S.-H. Kim, ‘‘A content service deployment plan for metaverse museum exhibitions—Centering on the combination of beacons and HMDs,’’ Int. J. Inf. Manage., vol. , no. , pp. –, [] S.-N. Suzuki, H. Kanematsu, D. M. Barry, N. Ogawa, K. Yajima, K. T. Nakahira, T. Shirai, M. Kawaguchi, T. Kobayashi, and M. Yoshitake, ‘‘Virtual experiments in metaverse and their applications to collaborative projects: The framework and its signiﬁcance,’’ Proc. Comput. Sci., vol. , pp. –, Jan. . [] B. Ryskeldiev, Y. Ochiai, M. Cohen, and J. Herder, ‘‘Distributed metaverse: Creating decentralized blockchain-based model for peer-to- peer sharing of virtual spaces for mixed reality applications,’’ in Proc. 9th Augmented Hum. Int. Conf., Feb. , pp. –. [] B. Kitchenham, O. P. Brereton, D. Budgen, M. Turner, J. Bailey, and S. Linkman, ‘‘Systematic literature reviews in software engineering—A systematic literature review,’’ Inf. Softw. Technol., vol. , no. , pp. –, [] N. Stephenson, Snowcrash. London, U.K.: ROC, . [] R. Schroeder, H. Avon Huxor, and S. Andy, ‘‘Activeworlds: Geography interaction in virtual reality,’’ Futures, vol. , no. , [] C. Jaynes, W. B. Seales, K. Calvert, Z. Fei, and J. Grifﬁoen, ‘‘The metaverse: A networked collection of inexpensive, self-conﬁguring, immersive environments,’’ in Proc. Workshop Virtual Environ. (EGVE), [] C. Ondrejka, ‘‘Escaping the gilded cage: User created content and building the metaverse,’’ NYL Sch. L. Rev., vol. , p. , May . ‘‘Human-level artiﬁcial general possibility of a technological singularity: A reaction to Ray Kurzweil’s the singularity is near, and McDermott’s critique of Kurzweil,’’ Artif. Intell., vol. , no. , pp. –, . [] C. Collins, ‘‘Looking to the future: Higher education in the Metaverse,’’ Educause Rev., vol. , no. , pp. –, . [] M. Wright, H. Ekeus, R. Coyne, J. Stewart, P. Travlou, and R. Williams, ‘‘Augmented duality: Overlapping a metaverse with the real world,’’ in Proc. Int. Conf. Adv. Comput. Entertainment Technol. (ACE), , [] E. Schlemmer, T. D. Trein, and O. Cristoffer, Telepresence in 3D avatar-driven digital-virtual worlds,’’ Tic. Revista d’innovaci? Educativa, vol. , pp. –, Jul. . [] F. M. Schaf, D. Müller, F. W. Bruns, C. E. Pereira, and H.-H. Erbe, ‘‘Collaborative learning and engineering workspaces,’’ Annu. Rev. Control, vol. , no. , pp. –, Dec. . [] G. Prisco, ‘‘A virtual world space agency,’’ Futures, vol. , no. , [] M. Rymaszewski, W. J. Au, M. Wallace, C. Winters, C. Ondrejka, and B. Batstone-Cunningham, Second Life: The Ofﬁcial Guide. Hoboken, NJ, [] P. R. Messinger, E. Stroulia, K. Lyons, M. Bone, R. H. Niu, K. Smirnov, and S. Perelgut, ‘‘Virtual worlds—Past, present, and future: New directions in social computing,’’ Decis. Support Syst., vol. , no. , [] S. Hazan, ‘‘Musing the metaverse,’’ in Heritage in the Digital Era. Brentwood, Esse, U.K.: Multi-Science Publishing, . [] S. Papagiannidis and M. Bourlakis, ‘‘Staging the new retail drama: At a metaverse near you!’’ J. Virtual Worlds Res., vol. , no. , pp. –, [] M. Forte, N. Lercari, F. Galeazzi, and D. Borra, ‘‘Metaverse communities and archaeology: The case of Teramo,’’ in Proc. EuroMed, Nov. , [] T. C. Cunningham, ‘‘Marching toward the metaverse; strategic com- munication through the new media,’’ Army Command Gen. Staff Coll Fort Leavenworth KS School Adv. Mil. Stud., VA, USA, Tech. [] D. Owens, A. Mitchell, D. Khazanchi, and I. Zigurs, ‘‘An empirical investigation of virtual world projects and metaverse technology capa- bilities,’’ ACM SIGMIS Database, Database Adv. Inf. Syst., vol. , no. , [] C. N. Tonéis, ‘‘Puzzles as a creative form of play in metaverse,’’ J. Virtual Worlds Res., vol. , no. , Jul. . [] J. Guo, C. Angelina, and W. T. Rolf, ‘‘Virtual wealth protection through virtual money exchange,’’ Electron. Commerce Res. Appl., vol. , no. , [] T. M. Connolly, M. Stansﬁeld, and T. Hainey, ‘‘An alternate reality game for language learning: ARGuing for multilingual motivation,’’ Comput. Educ., vol. , no. , pp. –, Aug. . [] A. Resmini and R. Luca, ‘‘Pervasive Information Architecture: Designing Cross-Channel User Experiences, vol. . Amsterdam, The Netherlands: [] F. Müller, ‘‘Remembering in the metaverse: Preservation, evaluation, and perception,’’ Ph.D. dissertation, Dept. Mathematik Informatik, Univ. Basel, Basel, Switzerland, . [] D. Xanthopoulou and S. Papagiannidis, ‘‘Play online, work better? Exam- ining the spillover of active learning and transformational leadership,’’ Technol. Forecasting Social Change, vol. , no. , pp. –, [] A. Cameron, ‘‘Splendid isolation: ‘Philosopher’s islands’ and the reimag- ination of space,’’ Geoforum, vol. , no. , pp. –, Jun. . [] I. Hughes, ‘‘Virtual worlds, augmented reality, blended reality,’’ Comput. Netw., vol. , no. , pp. –, Dec. . [] C. Kim, S.-G. Lee, and M. Kang, ‘‘I became an attractive person in the virtual world: Users’ identiﬁcation with virtual communities and avatars,’’ Comput. Hum. Behav., vol. , no. , pp. –, Sep. . [] H. Kanematsu, T. Kobayashi, N. Ogawa, D. M. Barry, Y. Fukumura, and H. Nagai, ‘‘Eco car project for Japan students as a virtual PBL class,’’ Proc. Comput. Sci., vol. , pp. –, Jan. . [] G. Kipper and R. Joseph, Augmented Reality: An Emerging Technologies Guide to AR. Amsterdam, The Netherlands: Elsevier, , ch. . [] S.-K. Kim, Y. S. Joo, M. Shin, S. Han, and J.-J. Han, ‘‘Virtual world control system using sensed information and adaptation engine,’’ Signal Process., Image Commun., vol. , no. , pp. –, Feb. . [] M. Preda, F. Morán, and C. Timmerer, ‘‘Introduction to the special issue on MPEG-V,’’ Signal Process., Image Commun., vol. , no. , pp. –, [] A. Luse, B. Mennecke, and J. Triplett, ‘‘The changing nature of user attitudes toward virtual world technology: A longitudinal study,’’ Comput. Hum. Behav., vol. , no. , pp. –, May . [] J. D. N. Dionisio, W. G. B. Lii, and R. Gilbert, ‘‘3D virtual worlds and the metaverse: Current status and future possibilities,’’ ACM Comput. Surv., vol. , no. , pp. –, Jun. . [] E. Ko and J. Jang, ‘‘The virtual device managing module of the metaverse assisted living support system,’’ in Proc. Int. Conf. Modeling, Simulation Vis. Methods (MSV) Steering Committee World Congr. Comput., Eng. Appl. Comput. (WorldComp), , pp. –. [] M.-I. Dascalu, A. Moldoveanu, and E. A. Shudayfat, ‘‘Mixed reality to support new learning paradigms,’’ in Proc. 18th Int. Conf. Syst. Theory, Control Comput. (ICSTCC), Oct. , pp. –. [] M. A. González, B. S. N. Santos, A. R. Vargas, J. Martín-Gutiérrez, and A. R. Orihuela, ‘‘Virtual worlds. Opportunities and challenges in the 21st century,’’ Proc. Comput. Sci., vol. , pp. –, Jan. . [] T. Amorim, L. Tapparo, N. Marranghello, A. C. R. Silva, and A. S. Pereira, ‘‘A multiple intelligences theory-based 3D virtual lab environment for digital systems teaching,’’ Proc. Comput. Sci., vol. , [] K. Yoon, S.-K. Kim, J. J. Han, S. Han, and M. Preda, MPEG-V: Bridging the Virtual and Real World. New York, NY, USA: Academic, S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges [] A. Moldoveanu, A. Gradinaru, O.-M. Ferche, and L. Stefan, ‘‘The 3D UPB mixed reality campus: Challenges of mixing the real and the virtual,’’ in Proc. 18th Int. Conf. Syst. Theory, Control Comput. (ICSTCC), [] T. Kwanya, S. Christine, and U. Peter, Library .: Intelligent Libraries and Apomediation. Amsterdam, The Netherlands: Elsevier, . [] D. M. Barry, N. Ogawa, A. Dharmawansa, H. Kanematsu, Y. Fukumura, T. Shirai, K. Yajima, and T. Kobayashi, ‘‘Evaluation for students’ learning manner using eye blinking system in metaverse,’’ Proc. Comput. Sci., vol. , pp. –, Jan. . [] S.-V. Rehm, L. Goel, and M. Crespi, ‘‘The metaverse as mediator between technology, trends, and the digital transformation of society and business,’’ J. Virtual Worlds Res., vol. , no. , pp. –, Oct. . [] J. C. Chen, ‘‘The crossroads of English language learners, task-based instruction, and 3D multi-user virtual learning in second life,’’ Comput. Educ., vol. , pp. –, Nov. . Shariatpanahi, M. M. Zolfagharzadeh, A. A. Pourezzat, ‘‘Toward a simulated replica of futures: Classiﬁcation and possible trajectories of simulation in futures studies,’’ Futures, [] H. Kanematsu, N. Ogawa, A. Shimizu, T. Shirai, M. Kawaguchi, T. Kobayashi, K. T. Nakahira, and D. M. Barry, ‘‘Skype discussion for PBL between two laboratories and students biological/psychological responses,’’ Proc. Comput. Sci., vol. , pp. –, Jan. . [] K. J. L. Nevelsteen, ‘‘Virtual world, deﬁned from a technological per- spective and applied to video games, mixed reality, and the metaverse,’’ Comput. Animation Virtual Worlds, vol. , no. , p. e1752, Jan. . really virtual: Towards a heritage metaverse,’’ Stud. Digit. Heritage, vol. , no. , pp. –, Jun. . [] P. De Decker and S. Peterson, ‘‘Beyond virtual or physical environments: Building a research metaverse a white paper for NDRIO’s Canadian digital research needs assessment,’’ Digit. Res. Alliance Canada, Toronto, [] A. Siyaev and G.-S. Jo, ‘‘Towards aircraft maintenance metaverse using speech interactions with virtual objects in mixed reality,’’ Sensors, vol. , [] M. Dowling, ‘‘Fertile LAND: Pricing non-fungible tokens,’’ Finance Res. Lett., Apr. , Art. no. . [] Metaverse Wiki. Accessed: Nov. , . https://en.wikipedia.org/wiki/Metaverse [] N. Stephenson, Snow Crash. New York, NY, USA: Bantam Books, . [] S. G. Lee, S. Trimi, W. K. Byun, and M. Kang, ‘‘Innovation and imitation effects in Metaverse service adoption,’’ Service Bus., vol. , no. , [] M. Grimshaw, The Oxford Handbook of Virtuality. New York, NY, USA: Oxford Univ. Press, , p. . [] Digital Twin Wiki. Accessed: Nov. , . [Online]. Available: https://en.wikipedia.org/wiki/Digital_twin [] K. Oddone, ‘‘Even better than the real thing?’’ Virtual Augmented Reality School Library, SCIS Connections, Educ. Service Australia, Melbourne, VIC, Australia, Tech. Rep., , pp. –, no. . [] S. Townsdin and W. Whitmer, ‘‘Implementing augmented reality in academic libraries,’’ Public Services Quart., vol. , pp. –, Jul. , doi: . [] M. Alcañiz, E. Bigné, and J. Guixeres, ‘‘Virtual reality in marketing: A framework, review, and research agenda,’’ Frontiers Psychol., vol. , p. , Jul. , doi: /. [] M. Alcañiz, E. Bigné, and J. Guixeres, ‘‘Virtual reality in marketing: A framework, review, and research agenda,’’ Frontiers Psychol., vol. , [] L. Birnie, T. Abhayapala, V. Tourbabin, and P. Samarasinghe, ‘‘Mixed source sound ﬁeld translation for virtual binaural application with perceptual validation,’’ IEEE/ACM Trans. Audio, Speech, Lang. Process., [] J. W. Ruffner, J. E. Fulbrook, and M. Foglia, ‘‘Near-to-eye display concepts for air trafﬁc controllers,’’ Proc. SPIE, vol. , pp. –, [] Z. Li, J. Chan, J. Walton, H. Benko, D. Wigdor, and M. Glueck, ‘‘Armstrong: An empirical examination of pointing at non-dominant arm-anchored UIs in virtual reality,’’ in Proc. CHI Conf. Hum. Factors Comput. Syst., May , pp. –. [] E. Bouzbib, G. Bailly, S. Haliyo, and P. Frey, ‘‘‘Can i touch this?’: Survey of virtual reality interactions via haptic solutions,’’ , [] C. R. Foy, J. J. Dudley, A. Gupta, H. Benko, and P. O. Kristensson, ‘‘Understanding, detecting and mitigating the effects of coactivations in ten-ﬁnger mid-air typing in virtual reality,’’ in Proc. CHI Conf. Hum. Factors Comput. Syst., May , pp. –. [] Z. Ren, I. Misra, A. G. Schwing, and R. Girdhar, ‘‘3D spatial recognition without spatially labeled 3D,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] Y. Li, W. Ouyang, B. Zhou, J. Shi, C. Zhang, and X. Wang, ‘‘Factorizable Net: An efﬁcient subgraph-based framework for scene graph generation,’’ in Proc. Eur. Conf. Comput. Vis. (ECCV), Sep. , pp. –. [] M. Chen, A. Thierry, and D. Ludovic, ‘‘Unsupervised object segmen- tation by redrawing,’’ in Proc. Adv. Neural Inf. Process. Syst., , [] R. J. Skerry-Ryan, E. Battenberg, Y. Xiao, Y. Wang, D. Stanton, J. Shor, R. Weiss, R. Clark, and R. A. Saurous, ‘‘Towards end-to-end prosody transfer for expressive speech synthesis with tacotron,’’ in Proc. Int. Conf. Mach. Learn., , pp. –. [] M. Fazel-Zarandi, S. Biswas, R. Summers, A. Elmalt, A. McCraw, M. McPhilips, and J. Peach, ‘‘Towards personalized dialog policies for conversational skill discovery,’’ in Proc. 33rd Conf. Neural Inf. Process. [] R. Zellers, M. Yatskar, S. Thomson, and Y. Choi, ‘‘Neural motifs: Scene graph parsing with global context,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. , pp. –. [] D. Mehta, O. Sotnychenko, F. Mueller, W. Xu, M. Elgharib, P. Fua, H.-P. Seidel, H. Rhodin, G. Pons-Moll, and C. Theobalt, ‘‘XNect: Real- time multi-person 3D motion capture with a single RGB camera,’’ ACM Trans. Graph., vol. , no. , p. , Jul. . [] M. Wang, X.-Q. Lyu, Y.-J. Li, and F.-L. Zhang, ‘‘VR content creation and exploration with deep learning: A survey,’’ Comput. Vis. Media, vol. , [] S. Poria, E. Cambria, R. Bajpai, and A. Hussain, ‘‘A review of affective computing: From unimodal analysis to multimodal fusion,’’ Inf. Fusion, vol. , pp. –, Sep. . [] Z. Ren, X. Wang, N. Zhang, X. Lv, and L.-J. Li, ‘‘Deep reinforcement learning-based image captioning with embedding reward,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. , pp. –. [] J. Lu, C. Xiong, D. Parikh, and R. Socher, ‘‘Knowing when to look: Adaptive attention via a visual sentinel for image captioning,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), vol. , Jul. , [] C. C. Park, B. Kim, and G. Kim, ‘‘Attend to you: Personalized image captioning with context sequence memory networks,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. , pp. –. [] A. Karpathy and L. Fei-Fei, ‘‘Deep visual-semantic alignments for generating image descriptions,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] L. Yang, K. Tang, J. Yang, and L.-J. Li, ‘‘Dense captioning with joint inference and visual context,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)), Jul. , pp. –. [] R. Vedantam, S. Bengio, K. Murphy, D. Parikh, and G. Chechik, ‘‘Context-aware captions from context-agnostic supervision,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. [] J. Fu, T. Mei, K. Yang, H. Lu, and Y. Rui, ‘‘Tagging personal photos with transfer deep learning,’’ in Proc. 24th Int. Conf. World Wide Web, Int. World Wide Web Conf. Steering Committee, May , pp. –. [] P. Rastogi, A. Poliak, V. Lyzinski, and B. Van Durme, ‘‘Neural variational entity set expansion for automatically populated knowledge graphs,’’ Inf. Retr. J., vol. , nos. –, pp. –, Aug. . [] Y. Fu, Y. Feng, and J. P. Cunningham, ‘‘Paraphrase generation with latent bag of words,’’ in Proc. Adv. Neural Inf. Process. Syst., , [] D. Zeng, H. Zhang, L. Xiang, J. Wang, and G. Ji, ‘‘User-oriented paraphrase generation with keywords controlled network,’’ IEEE Access, [] Y. Ma, K. L. Nguyen, F. Z. Xing, and E. Cambria, ‘‘A survey on empathetic dialogue systems,’’ Inf. Fusion, vol. , pp. –, Dec. . [] Y. Zheng, G. Chen, M. Huang, S. Liu, and X. Zhu, ‘‘Persona-aware dialogue generation with enriched proﬁle,’’ in Proc. 33rd Conf. Neural Inf. Process. Syst. (NIPS), Vancouver, BC, Canada, , pp. –. [] N. Lubis, S. Sakti, K. Yoshino, and S. Nakamura, ‘‘Dialogue model and response generation for emotion improvement elicitation,’’ in Proc. 33rd Conf. Neural Inf. Process. Syst. (NIPS), , pp. –. S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges [] K. Chandu, S. Prabhumoye, R. Salakhutdinov, and A. W. Black, ‘‘‘My way of telling a story’: Persona based grounded story generation,’’ in Proc. 2nd Workshop Storytelling, , pp. –. [] CTRL—A Conditional Transformer Language Model for Controllable Generation GitHub. Accessed: Nov. , . [Online]. Available: https://github.com/salesforce/ctrl [] C. Moon, P. Jones, and N. F. Samatova, ‘‘Learning entity type embeddings for knowledge graph completion,’’ in Proc. ACM Conf. Inf. Knowl. Manage., Nov. , pp. –. [] L. Poddar, W. Hsu, and M. L. Lee, ‘‘Author-aware aspect topic sentiment model to retrieve supporting opinions from reviews,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., , pp. –. [] M. Zhang, Y. Zhang, and G. Fu, ‘‘End-to-end neural relation extraction with global optimization,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., , pp. –. [] F. Liu, B. Liu, C. Sun, M. Liu, and X. Wang, ‘‘Multimodal learning-based approaches for link prediction in social networks,’’ in Natural Language Processing and Chinese Computing. Cham, Switzerland: Springer, , [] Q. Ning, Z. Feng, and D. Roth, ‘‘A structured learning approach to temporal relation extraction,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., , pp. –. [] Q. Huang, Z. Gan, A. Celikyilmaz, D. Wu, J. Wang, and X. He, ‘‘Hierarchically structured reinforcement learning for topically coherent visual story generation,’’ in Proc. AAAI Conf. Artif. Intell., vol. , , [] F. Liu, B. Liu, C. Sun, M. Liu, and X. Wang, ‘‘Multimodal deep belief network based link prediction and user comment generation,’’ in Proc. Int. Conf. Neural Inf. Process. Cham, Switzerland: Springer, Nov. , [] C. Ciliberto, A. Rudi, L. Rosasco, and M. Pontil, ‘‘Consistent multitask learning with nonlinear output relations,’’ in Proc. Adv. Neural Inf. Process. Syst., , pp. –. [] W. Zhuo, M. Salzmann, X. He, and M. Liu, ‘‘Indoor scene parsing with instance segmentation, semantic labeling and support relationship inference,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), [] P. K. Choubey and R. Huang, ‘‘A sequential model for classifying temporal relations between intra-sentence events,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., , pp. –. [] S. Woo, D. Kim, D. Cho, and I. S. Kweon, ‘‘LinkNet: Relational embedding for scene graph,’’ in Proc. Adv. Neural Inf. Process. Syst., [] L. Xiao, X. Hu, Y. Chen, Y. Xue, D. Gu, B. Chen, and T. Zhang, ‘‘Targeted sentiment classiﬁcation based on attentional encoding and graph convolutional networks,’’ Appl. Sci., vol. , no. , p. , [] Y. Wei, X. Wang, L. Nie, X. He, R. Hong, and T.-S. Chua, ‘‘MMGCN: Multi-modal graph convolution network for personalized recommen- dation of micro-video,’’ in Proc. 27th ACM Int. Conf. Multimedia, [] E. Zuo, H. Zhao, B. Chen, and Q. Chen, ‘‘Context-speciﬁc heterogeneous graph convolutional network for implicit sentiment analysis,’’ IEEE Access, vol. , pp. –, . [] M. Kocaoglu, C. Snyder, A. G. Dimakis, and S. Vishwanath, ‘‘Causal- implicit generative models with adversarial training,’’ in Proc. Int. Conf. Learn. Represent. (ICLR), , pp. –. [] H. Nam, J.-W. Ha, and J. Kim, ‘‘Dual attention networks for multimodal reasoning and matching,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. , pp. –. [] X. Geng, H. Zhang, Z. Song, Y. Yang, H. Luan, and T.-S. Chua, ‘‘One of a kind: User proﬁling by social curation,’’ in Proc. 22nd ACM Int. Conf. Multimedia, Nov. , pp. –. [] L. Gui, J. Hu, Y. He, R. Xu, L. Qin, and J. Du, ‘‘A question answering approach for emotion cause extraction,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., , pp. –. [] F. Yang, Z. Yang, and W. W. Cohen, ‘‘Differentiable learning of logical rules for knowledge base reasoning,’’ in Proc. Adv. Neural Inf. Process. [] H. Lin, L. Sun, and X. Han, Proc. Conf. Empirical Methods Natural Lang. Process., , [] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh, ‘‘Cluster-GCN: An efﬁcient algorithm for training deep and large graph convolutional networks,’’ in Proc. 25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Jul. , pp. –. [] C. Sun, Y. Gong, Y. Wu, M. Gong, D. Jiang, M. Lan, S. Sun, and N. Duan, ‘‘Joint type inference on entities and relations via graph convolutional networks,’’ in Proc. 57th Annu. Meeting Assoc. Comput. Linguistics, [] P. Tambwekar, M. Dhuliawala, L. J. Martin, A. Mehta, B. Harrison, and M. O. Riedl, ‘‘Controllable neural story plot generation via reward shaping,’’ in Proc. 28th Int. Joint Conf. Artif. Intell., Aug. , pp. –. [] M. Isonuma, T. Fujino, J. Mori, Y. Matsuo, and I. Sakata, ‘‘Extractive summarization using multi-task learning with document classiﬁcation,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., , [] H. Yu and M. O. Riedl, ‘‘A sequential recommendation approach for interactive personalized story generation,’’ in Proc. 11th Int. Conf. Auto. Agents Multiagent Syst., vol. , Jun. , pp. –. [] C. Gulcehre, F. Dutil, A. Trischler, and Y. Bengio, ‘‘Plan, attend, generate: Planning for sequence-to-sequence models,’’ in Proc. Adv. Neural Inf. Process. Syst., , pp. –. [] R. Paulus, C. Xiong, and R. Socher, ‘‘A deep reinforced model for abstractive summarization,’’ in Proc. Int. Conf. Learn. Represent. (ICLR), [] C. J. Nan, K. M. Kim, and B. T. Zhang, ‘‘Social network analysis of TV drama characters via deep concept hierarchies,’’ in Proc. Adv. Social Netw. Anal. Mining (ASONAM), IEEE/ACM Int. Conf., Aug. , pp. –. [] A. Newell and J. Deng, ‘‘Pixels to graphs by associative embedding,’’ in Proc. Adv. Neural Inf. Process. Syst., , pp. –. [] H. Hu, G.-T. Zhou, Z. Deng, Z. Liao, and G. Mori, ‘‘Learning structured inference neural networks with label relations,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] C. Xiang, T. Jiang, B. Chang, and Z. Sui, ‘‘ERSOM: A structural ontology matching approach using automatically learned entity representation,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., , [] P. Chen, Z. Sun, L. Bing, and W. Yang, ‘‘Recurrent attention network on memory for aspect sentiment analysis,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., , pp. –. [] V. Niculae and M. Blondel, ‘‘A regularized framework for sparse and structured neural attention,’’ in Proc. Adv. Neural Inf. Process. Syst., , [] L. Baraldi, C. Grana, and R. Cucchiara, ‘‘Recognizing and presenting the storytelling video structure with deep multimodal networks,’’ IEEE Trans. Multimedia, vol. , no. , pp. –, May . [] M. Bolaños, M. Dimiccoli, and P. Radeva, ‘‘Toward storytelling from visual lifelogging: An overview,’’ IEEE Trans. Human-Mach. Syst., vol. , no. , pp. –, Feb. . [] Y. Li, Z. Gan, Y. Shen, J. Liu, Y. Cheng, Y. Wu, L. Carin, D. Carlson, and J. Gao, ‘‘StoryGAN: A sequential conditional GAN for story visualization,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), [] H. Zhang, Z. Kyaw, S. F. Chang, and T. S. Chua, ‘‘Visual translation embedding network for visual relation detection,’’ in Proc. CVPR, vol. , [] L. Yu, M. Bansal, and T. Berg, ‘‘Hierarchically-attentive RNN for album summarization and storytelling,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., , pp. –. [] Q. Sun, S. Lee, and D. Batra, ‘‘Bidirectional beam search: Forward- backward inference in neural sequence models for ﬁll-in-the-blank image captioning,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), [] R. Hu, M. Rohrbach, J. Andreas, T. Darrell, and K. Saenko, ‘‘Modeling relationships in referential expressions with compositional modular networks,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), [] X. Wang, W. Chen, J. Wu, Y.-F. Wang, and W. Y. Wang, ‘‘Video captioning via hierarchical reinforcement learning,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. , pp. –. [] J. Tang, ‘‘Show, reward, and tell: Adversarial visual story generation,’’ ACM Trans. Multimedia Comput., Commun., Appl., vol. , no. 2s, [] J. Novikova, O. Dušek, A. C. Curry, and V. Rieser, ‘‘Why we need new evaluation metrics for NLG,’’ Proc. Conf. Empirical Methods Natural Lang. Process., Copenhagen, Denmark, Sep. , pp. –. [] L. Huang, K. Zhao, and M. Ma, ‘‘When to ﬁnish? Optimal beam search for neural text generation (modulo beam size),’’ in Proc. Conf. Empirical Methods Natural Lang. Process., , pp. –. [] J. Eisenberg and M. Finlayson, ‘‘A simpler and more generalizable story detector using verb and character features,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., , pp. –. S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges [] S. Swayamdipta, A. P. Parikh, and T. Kwiatkowski, ‘‘Multi-mention learning for reading comprehension with neural cascades,’’ in Proc. Int. Conf. Learn. Represent. (ICLR), , pp. –. [] G. A. B. Barros, M. C. Green, A. Liapis, and J. Togelius, ‘‘Who killed Albert Einstein? From open data to murder mystery games,’’ IEEE Trans. Games, vol. , no. , pp. –, Mar. . [] L. Bounegru, T. Venturini, J. Gray, and M. Jacomy, networks: Exploring the affordances of networks as storytelling devices in journalism,’’ Digit. J., vol. , no. , pp. –, . [] W. Y. Wang, Y. Mehdad, D. R. Radev, and A. Stent, ‘‘A low-rank approximation approach to learning joint embeddings of news stories and images for timeline summarization,’’ in Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol., , pp. –. [] L. Shi and R. Setchi, ‘‘User-oriented ontology-based clustering of stored memories,’’ Expert Syst. Appl., vol. , no. , pp. –, [] Y.-G. Cheong, Y.-J. Kim, W.-H. Min, E.-S. Shim, and J.-Y. Kim, ‘‘Prism: A framework for authoring interactive narratives,’’ in Proc. Joint Int. Conf. Interact. Digit. Storytelling. Berlin, Germany: Springer, , [] I. Subašić and B. Bettina, ‘‘Experience STORIES: A visual news search and summarization system,’’ in Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases. Berlin, Germany: Springer, , [] M. Krstajić, M. Najm-Araghi, F. Mansmann, and D. A. Keim, ‘‘Story tracker: Incremental visual text analytics of news story development,’’ Inf. Visualizat., vol. , nos. –, pp. –, Jul. . [] A. Miller, W. Feng, D. Batra, A. Bordes, A. Fisch, J. Lu, D. Parikh, and J. Weston, ‘‘ParlAI: A dialog research software platform,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., Syst. Demonstrations, , [] A. Das, S. Kottur, J. M. F. Moura, S. Lee, and D. Batra, ‘‘Learning cooperative visual dialog agents with deep reinforcement learning,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. , pp. –. [] J. A. Mendez, A. Geramifard, M. Ghavamzadeh, and B. Liu, ‘‘Reinforce- ment learning of multi-domain dialog policies via action embeddings,’’ in Proc. 3rd Workshop Conversational AI Today’s Practice Tomorrow’s Potential (NIPS), , pp. –. [] J. Wu, G. Li, S. Liu, and L. Lin, ‘‘Tree-structured policy based progressive reinforcement learning for temporally language grounding in video,’’ in Proc. AAAI Conf. Artif. Intell., vol. , no. , Apr. , [] G. Gkioxari, R. Girshick, P. Dollar, and K. He, ‘‘Detecting and interactions,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. , pp. –. [] K. Nguyen and H. Daumé III, ‘‘Help, anna! Visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning,’’ in Proc. Conf. Empirical Methods Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process. (EMNLP-IJCNLP), , [] Y. Jiang, S. Gu, K. Murphy, and C. Finn, ‘‘Language as an abstraction for hierarchical deep reinforcement learning,’’ in Proc. Advances Neural Inf. Process. Syst., , pp. –. [] S. W. Lee, T. Gao, S. Yang, J. Yoo, and J. W. Ha, ‘‘Large-scale answerer in questioner’s mind for visual dialog question generation,’’ in Proc. ICLR, [] P. Ammanabrolu and M. Hausknecht, ‘‘Graph constrained reinforcement learning for natural language action spaces,’’ in Proc. ICRL, , [] T. Domhan and F. Hieber, ‘‘Using target-side monolingual data for neural machine translation through multi-task learning,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., , pp. –. [] D. Camacho, Á. Panizo-LLedot, G. Bello-Orgaz, A. Gonzalez-Pardo, and E. Cambria, ‘‘The four dimensions of social network analysis: An overview of research methods, applications, and software tools,’’ Inf. Fusion, vol. , pp. –, Nov. . [] J. Kruk, J. Lubin, K. Sikka, X. Lin, D. Jurafsky, and A. Divakaran, ‘‘Integrating text and image: Determining multimodal document intent in Instagram posts,’’ Proc. Conf. Empirical Methods Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process., , [] S. Palaskar, J. Libovický, S. Gella, and F. Metze, ‘‘Multimodal abstractive summarization for How2 videos,’’ in Proc. 57th Annu. Meeting Assoc. Comput. Linguistics, , pp. –. [] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, and J. Gao, ‘‘Uniﬁed vision-language pre-training for image captioning and VQA,’’ in Proc. AAAI Conf. Artif. Intell., vol. , no. , Apr. , pp. –. [] M. Hessel, H. Soyer, L. Espeholt, W. Czarnecki, S. Schmitt, and H. van Hasselt, ‘‘Multi-task deep reinforcement learning with popart,’’ in Proc. AAAI Conf. Artif. Intell., vol. , , pp. –. [] Y. Jia, R. J. Weiss, F. Biadsy, W. Macherey, M. Johnson, Z. Chen, and Y. Wu, ‘‘Direct speech-to-speech translation with a sequence-to-sequence model,’’ in Proc. Interspeech, Sep. , pp. –. [] Y. Qian, R. Ubale, V. Ramanaryanan, P. Lange, D. Suendermann-Oeft, K. Evanini, and E. Tsuprun, ‘‘Exploring ASR-free end-to-end modeling to improve spoken language understanding in a cloud-based dialog sys- tem,’’ in Proc. IEEE Automat. Speech Recognit. Understand. Workshop (ASRU), Dec. , pp. –. [] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, ‘‘Embodied question answering,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. , pp. –. [] M. Poesio, S. Roland, and V. Yannick, Anaphora Resolution. Cham, [] S. Shekhar, U. Kumar, and U. Sharma, ‘‘To reduce the multidimen- sionality of feature set for anaphora resolution algorithm,’’ in Ambient Communications and Computer Systems. Singapore: Springer, , [] Y. Niu, H. Zhang, M. Zhang, J. Zhang, Z. Lu, and J.-R. Wen, ‘‘Recursive visual attention in visual dialog,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] R. Sukthanker, S. Poria, E. Cambria, and R. Thirunavukarasu, ‘‘Anaphora and coreference resolution: A review,’’ Inf. Fusion, vol. , pp. –, [] A. Rohrbach, M. Rohrbach, S. Tang, S. J. Oh, and B. Schiele, ‘‘Generating descriptions with grounded and co-referenced people,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. , pp. –. [] B. Aktaş, T. Schefﬂer, and M. Stede, ‘‘Anaphora resolution for Twitter conversations: An exploratory study,’’ in Proc. 1st Workshop Comput. Models Reference, Anaphora Coreference, , pp. –. [] S. Heinrich, M. Kerzel, E. Strahl, and S. Wermter, multimodal interaction in language learning: The emil data collection,’’ in Proc. ICDL-EpiRob Workshop Active Vis., Attention, Learn. (ICDL- Epirob AVAL)(Tokyo), vol. , , pp. –. [] Y. Jiang, W. Li, M. S. Hossain, M. Chen, A. Alelaiwi, and M. Al-Hammadi, ‘‘A snapshot research and implementation of mul- timodal information fusion for data-driven emotion recognition,’’ Inf. Fusion, vol. , pp. –, Jan. . [] J. Zhang, K. Shih, A. Tao, B. Catanzaro, and A. Elgammal, ‘‘An interpretable model for scene graph generation,’’ in Proc. Adv. Neural Inf. [] P. P. Liang, A. Zadeh, and L.-P. Morency, ‘‘Multimodal local-global ranking fusion for emotion recognition,’’ in Proc. 20th ACM Int. Conf. Multimodal Interact., Oct. , pp. –. [] K. Ethayarajh, ‘‘How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT- embeddings,’’ in Proc. Conf. Empirical Methods Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process. (EMNLP-IJCNLP), , pp. –. [] Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou, ‘‘MobileBERT: A compact task-agnostic BERT for resource-limited devices,’’ in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, , pp. –. [] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, and S. Agarwal, ‘‘Language models are few-shot learners,’’ , arXiv:.. [] H. Tan and M. Bansal, ‘‘Vokenization: Improving language understanding with contextualized, visual-grounded supervision,’’ Empirical Methods Natural Lang. Process. (EMNLP), , pp. –. [] T5: Text-to-Text Transfer Transformer GitHub. Accessed: Nov. , . https:github.com/google-research/text-to-text- [] H. Le, D. Sahoo, N. Chen, and S. Hoi, ‘‘Multimodal transformer networks for end-to-end video-grounded dialogue systems,’’ in Proc. 57th Annu. Meeting Assoc. Comput. Linguistics, , pp. –. [] J. Lu, D. Batra, D. Parikh, and S. Lee, ‘‘Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks,’’ in Adv. Neural Inf. Process. Syst., , pp. –. [] Z. Zheng, W. Wang, S. Qi, and S.-C. Zhu, ‘‘Reasoning visual dialogs with structural and partial observations,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges [] A. S. Gordon, C. A. Bejan, and K. Sagae, ‘‘Commonsense causal reasoning using millions of personal stories,’’ in Proc. 35th AAAI Conf. [] D. Wang, M. Jamnik, and P. Lio, ‘‘Abstract diagrammatic reasoning with multiplex graph networks,’’ in Proc. ICRL, , pp. –. [] A. Asai, K. Hashimoto, H. Hajishirzi, R. Socher, and C. Xiong, ‘‘Learning to retrieve reasoning paths over wikipedia graph for question answering,’’ [] W. Xiong, T. Hoang, and W. Y. Wang, ‘‘DeepPath: A reinforcement learning method for knowledge graph reasoning,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., , pp. –. [] H. Zhu, Y. Lin, Z. Liu, J. Fu, T.-S. Chua, and M. Sun, ‘‘Graph neural networks with generated parameters for relation extraction,’’ in Proc. 57th Annu. Meeting Assoc. Comput. Linguistics, , pp. –. [] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V. Prasanna, ‘‘Graphsaint: Graph sampling based inductive learning method,’’ in Proc. [] T. Bansal, D.-C. Juan, S. Ravi, and A. McCallum, ‘‘A2N: Attending to neighbors for knowledge graph inference,’’ in Proc. 57th Annu. Meeting Assoc. Comput. Linguistics, , pp. –. [] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine, ‘‘Diversity is all you need: Learning skills without a reward function,’’ in Proc. ICLR, , [] M. Laskin, A. Srinivas, and P. Abbeel, ‘‘CURL: Contrastive unsupervised representations for reinforcement learning,’’ in Proc. Int. Conf. Mach. [] X. Puig, T. Shu, S. Li, Z. Wang, Y.-H. Liao, J. B. Tenenbaum, S. Fidler, and A. Torralba, ‘‘Watch- and-help: A challenge for social perception and human-AI collaboration,’’ in Proc. ICLR, , pp. –. [] J. B. Hamrick, K. R. Allen, V. Bapst, T. Zhu, K. R. McKee, J. B. Tenenbaum, and P. W. Battaglia, ‘‘Relational inductive bias for physical construction in humans and machines,’’ in Proc. Annu. Meeting Cogn. Sci. Soc. (CogSci), , pp. –. [] J. B. Hamrick, ‘‘Analogues of mental simulation and imagination in deep learning,’’ Current Opinion Behav. Sci., vol. , pp. –, Oct. . [] C. Davis, L. Bulat, A. Vero, and E. Shutova, ‘‘Modelling visual properties and visual context in multimodal semantics,’’ in Proc. Adv. Neural Inf. [] W. Dabney, Z. Kurth-Nelson, N. Uchida, C. K. Starkweather, D. Hassabis, R. Munos, and M. Botvinick, ‘‘A distributional code for value in dopamine-based reinforcement learning,’’ Nature, vol. , [] M. Moscovitch, R. Cabeza, G. Winocur, and L. Nadel, ‘‘Episodic memory and beyond: The hippocampus and neocortex in transformation,’’ Annu. Rev. Psychol., vol. , pp. –, Jan. . [] D. Lopez-Paz and M. Ranzato, ‘‘Gradient episodic memory for continual learning,’’ in Proc. Adv. Neural Inf. Process. Syst., , pp. –. [] P. Y. Oudeyer, J. Gottlieb, and M. Lopes, ‘‘Intrinsic motivation, curiosity, and learning: Theory and applications in educational technologies,’’ Prog. Brain Res., vol. , pp. –, Jan. . [] N. C. Rabinowitz, F. Perbet, F. Song, C. Zhang, S. M. A. Eslami, and M. Botvinick, ‘‘Machine theory of mind,’’ in Proc. 35th Int. Conf. Mach. Learn., Stockholm, Sweden, vol. , , pp. –. [] D. Melhart, G. N. Yannakakis, and A. Liapis, ‘‘I feel i feel you: A theory of mind experiment in games,’’ KI Künstliche Intelligenz, vol. , no. , [] A. Raj, J. Tanke, J. Hays, M. Vo, C. Stoll, and C. Lassner, ‘‘ANR: Articulated neural rendering for virtual avatars,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] L. Liu, T. Zhou, G. Long, J. Jiang, and C. Zhang, ‘‘Learning to propagate for graph meta-learning,’’ in Proc. Adv. Neural Inf. Process. Syst., , [] R. Vuorio, S. H. Sun, H. Hu, and J. J. Lim, ‘‘Multimodal model-agnostic meta-learning via task-aware modulation,’’ in Proc. Adv. Neural Inf. Process. Syst., , pp. –. [] S. Racanière, T. Weber, D. P. Reichert, L. Buesing, A. Guez, D. Rezende, A. P. Badia, O. Vinyals, N. Heess, Y. Li, and R. Pascanu, ‘‘Imagination- augmented agents for deep reinforcement learning,’’ in Proc. Adv. Neural Inf. Process. Syst., , pp. –. [] R. Skarbez, M. Smith, and M. C. Whitton, ‘‘Revisiting milgram and Kishino’s reality-virtuality continuum,’’ Frontiers Virtual Reality, vol. , [] P. Maharg and M. Owen, ‘‘Simulations, learning and the metaverse: Changing cultures in legal education,’’ J. Inf., Law, Technol., vol. , [] J. Shi, T. Honjo, K. Zhang, and K. Furuya, ‘‘Using virtual reality to assess landscape: A comparative study between on-site survey and virtual reality of aesthetic preference and landscape cognition,’’ Sustainability, vol. , [] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi, ‘‘IQA: Visual question answering in interactive environ- ments,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., [] Y. Qiu, A. Pal, and H. I. Christensen, ‘‘Target driven visual nav- relationships,’’ Tech. Rep., Mar. . [Online]. Available: https://arxiv.org/pdf/ [] J. Li, S. Tang, F. Wu, and Y. Zhuang, ‘‘Walking with MIND: Mental imagery eNhanceD embodied QA,’’ in Proc. 27th ACM Int. Conf. Multimedia, Oct. , pp. –. [] R. Tamari, C. Shani, T. Hope, M. R. L. Petruck, O. Abend, and D. Shahaf, ‘‘Language (Re)modelling: Towards embodied language under- standing,’’ in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, , [] B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell, B. McGrew, and I. Mordatch, ‘‘Emergent tool use from multi-agent autocurricula,’’ in Proc. Int. Conf. Learn. Represent., , pp. –. [] N. Ilinykh, S. Zarrieß, and D. Schlangen, ‘‘Meetup! A corpus of joint activity dialogues in a visual environment,’’ in Proc. 23rd Workshop Semantics Pragmatics Dialogue (Semdial/LondonLogue), London, U.K., [] B. Chen, S. Song, H. Lipson, and C. Vondrick, Visual Hide and Seek (Artiﬁcial Life Conference Proceedings). Cambridge, MA, USA: MIT [] I.-C. Stanica, F. Moldoveanu, G.-P. Portelli, M.-I. Dascalu, A. Moldoveanu, and M. G. Ristea, ‘‘Flexible virtual reality system for neurorehabilitation and quality of life improvement,’’ Sensors, vol. , no. , p. , Oct. . [] S. Papagiannidis, M. Bourlakis, and F. Li, ‘‘Making real money in virtual worlds: MMORPGs and emerging business opportunities, challenges and ethical implications in Metaverses,’’ Technol. Forecasting Social Change, vol. , no. , pp. –, . [] J. Smart, J. Cascio, J. Paffendorf, C. Bridges, J. Hummel, J. Hursthouse, and R. Moss, ‘‘A cross-industry public foresight project,’’ in Proc. Metaverse Roadmap Pathways 3DWeb, , pp. –. [] Y. Tang, ‘‘Help ﬁrst-year college students to learn their library through an augmented reality game,’’ J. Academic Librarianship, vol. , no. , [] M. Noghabaei, A. Heydarian, V. Balali, and K. Han, ‘‘A survey study to understand industry vision for virtual and augmented reality applications in design and construction,’’ , arXiv:.. [] H. Kanematsu, T. Kobayashi, D. M. Barry, Y. Fukumura, A. Dharmawansa, and N. Ogawa, ‘‘Virtual STEM class for nuclear safety education in metaverse,’’ Proc. Comput. Sci., vol. , pp. –, [] B. Sung, E. Mergelsberg, M. Teah, B. D’Silva, and I. Phau, ‘‘The effectiveness of a marketing virtual reality learning simulation: A quantitative survey with psychophysiological measures,’’ Brit. J. Educ. Technol., vol. , no. , pp. –, Jan. . [] T. Templeton, ‘‘Getting real: Learning with (and about) augmented reality,’’ Scan, J. Educators, vol. , no. , pp. –, . [] D. M. Barry, H. Kanematsu, Y. Fukumura, N. Ogawa, A. Okuda, R. Taguchi, and H. Nagai, ‘‘International comparison for problem based learning in metaverse,’’ in Proc. ICEE ICEER, vol. , , pp. –. [] H. Kanematsu, Y. Fukumura, N. Ogawa, A. Okuda, R. Taguchi, and H. Nagai, ‘‘Practice and evaluation of problem based learning in metaverse,’’ in Proc. EdMediaC Innovate Learn. Assoc. Advancement Comput. Educ. (AACE), , pp. –. [] N. Khan, K. Muhammad, T. Hussain, M. Nasir, M. Munsif, A. S. Imran, and M. Sajjad, ‘‘An adaptive game-based learning strategy for children road safety education and practice in virtual space,’’ Sensors, vol. , [] Afnan, K. Muhammad, N. Khan, M.-Y. Lee, A. Imran, and M. Sajjad, ‘‘School of the future: A comprehensive study on the effectiveness of augmented reality as a tool for primary school Children’s education,’’ Appl. Sci., vol. , no. , p. , Jun. . [] Y. Li, T. Nagarajan, B. Xiong, and K. Grauman, ‘‘Ego-Exo: Transferring visual representations from third-person to ﬁrst-person videos,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges [] W. Xian, J.-B. Huang, J. Kopf, and C. Kim, ‘‘Space-time neural irradiance ﬁelds for free-viewpoint video,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] R. Bonatti, A. Bucker, S. Scherer, M. Mukadam, and J. Hodgins, ‘‘Batteries, camera, action! Learning a semantic control space for expressive robot cinematography,’’ , arXiv:.. [] H. Zhang, Y. Ye, T. Shiratori, and T. Komura, ‘‘ManipNet: Neural manipulation synthesis with a hand-object spatial representation,’’ ACM Trans. Graph., vol. , no. , pp. –, Aug. . [] C. Rognon, T. Bunge, M. Gao, C. Connor, B. Stephens-Fripp, C. Brown, and A. Israr, ‘‘An online survey on the perception of mediated social touch interaction and device design,’’ , arXiv:.. [] E. D. Z. Chase, A. Israr, P. Preechayasomboon, S. Sykes, A. Gupta, and J. Hartcher-O’Brien, ‘‘Learning vibes: Communication bandwidth of a single wrist-worn vibrotactile actuator,’’ in Proc. IEEE World Haptics Conf. (WHC), Jul. , pp. –. [] B. Stephens-Fripp, A. Israr, and C. Rognon, ‘‘A multichannel pneumatic analog control system for haptic displays: Multichannel pneumatic analog control system (MPACS),’’ in Proc. Extended Abstr. CHI Conf. Hum. Factors Comput. Syst., May , pp. –. [] Y. Yuan, S.-E. Wei, T. Simon, K. Kitani, and J. Saragih, ‘‘SimPoE: Simulated character control for 3D human pose estimation,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , [] N. Neverova, A. Sanakoyeu, P. Labatut, D. Novotny, and A. Vedaldi, ‘‘Discovering relationships between object categories via universal canonical maps,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. ‘‘Chromatic-luminance and true-texture analysis for image mining in the social sciences,’’ Meta, CA, USA, Tech. Rep., Mar. . Accessed: Nov. , . [Online]. Avail- able: https://research.facebook.com/publications/chromatic-luminance- and-true-texture-analysis-for-image-mining-in-the-social-sciences/ [] S. d’Ascoli, H. Touvron, M. Leavitt, A. Morcos, G. Biroli, and L. Sagun, ‘‘ConViT: Improving vision transformers with soft convolutional inductive biases,’’ , arXiv:.. [] L. Porzi, S. R. Bulo, and P. Kontschieder, ‘‘Improving panoptic segmentation at all scales,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] B. Cheng, R. Girshick, P. Dollar, A. C. Berg, and A. Kirillov, ‘‘Boundary IoU: Improving object-centric image segmentation evaluation,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , [] Y. Jin, A. Patney, and A. Bovik, ‘‘Evaluating foveated video quality using entropic differencing,’’ , arXiv:.. [] B. Xiong, H. Fan, K. Grauman, and C. Feichtenhofer, ‘‘Multiview pseudo-labeling for semi-supervised learning from video,’’ , [] J. Huang, G. Pang, R. Kovvuri, M. Toh, K. J. Liang, P. Krishnan, X. Yin, and T. Hassner, ‘‘A multiplexed network for end-to-end, multilingual OCR,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] A. Singh, G. Pang, M. Toh, J. Huang, W. Galuba, and T. Hassner, ‘‘TextOCR: Towards large-scale end-to-end reasoning for arbitrary- shaped scene text,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] S. Sodhani, A. Zhang, and J. Pineau, ‘‘Multi-task reinforcement learning with context-based representations,’’ , arXiv:.. [] P. Henzler, J. Reizenstein, P. Labatut, R. Shapovalov, T. Ritschel, A. Vedaldi, and D. Novotny, ‘‘Unsupervised learning of 3D object categories from videos in the wild,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] Y.-C. Liu, C.-Y. Ma, Z. He, C.-W. Kuo, K. Chen, P. Zhang, B. Wu, Z. Kira, and P. Vajda, ‘‘Unbiased teacher for semi-supervised object detection,’’ [] X. Chen and K. He, ‘‘Exploring simple Siamese representation learning,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), [] Y. Tian, X. Chen, and S. Ganguli, ‘‘Understanding self-supervised learning dynamics without contrastive pairs,’’ , arXiv:. [] J. Donley, V. Tourbabin, B. Rafaely, and R. Mehra, ‘‘Adaptive multi- channel signal enhancement based on multi-source contribution estima- tion,’’ in Proc. 29th Eur. Signal Process. Conf. (EUSIPCO), Aug. , [] P. Tzirakis, A. Kumar, and J. Donley, ‘‘Multi-channel speech enhance- ment using graph neural networks,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Jun. , pp. –. [] H. Helmholz, J. Ahrens, D. L. Alon, S. V. A. Gari, and R. Mehra, ‘‘Evaluation of sensor self-noise in binaural rendering of spherical microphone array signals,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), May , pp. –. [] S. E. Chazan, L. Wolf, E. Nachmani, and Y. Adi, ‘‘Single channel voice separation for unknown number of speakers under reverberant and noisy settings,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Jun. , pp. –. [] T. Shlomo and B. Rafaely, ‘‘Blind amplitude estimation of early room reﬂections using alternating least squares,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Jun. , pp. –. [] T. Shlomo and B. Rafaely, ‘‘Blind localization of early room reﬂections using phase aligned spatial correlation,’’ IEEE Trans. Signal Process., [] Y. Zhou, H. Jiang, and V. K. Ithapu, ‘‘On the predictability of HRTFs from ear shapes using deep networks,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Jun. , pp. –. [] Z. Ben-Hur, D. L. Alon, R. Mehra, and B. Rafaely, reproduction based on bilateral ambisonics and ear-aligned HRTFs,’’ IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. , pp. –, [] C. Bartlomiej, T. C. Sang-Ik, and M. Ravish, ‘‘Full range omnidirectional sound source for near-ﬁeld head-related transfer-functions measure- ment,’’ J. Audio Eng. Soc., vol. , no. , pp. –, May . [] S. A. Gari, J. M. Arend, P. Calamia, and P. Robinson, ‘‘Optimizing the spatial decomposition method for binaural rendering,’’ J. Audio Eng. Soc., vol. , no. , pp. –, Dec. . [] S. Ge, V. Goswami, C. Lawrence Zitnick, and D. Parikh, ‘‘Creative sketch generation,’’ , arXiv:.. [] B. Roziere, N. C. Rakotonirina, V. Hosu, A. Rasoanaivo, H. Lin, C. Couprie, and O. Teytaud, ‘‘Tarsier: Evolving noise injection in super- resolution GANs,’’ in Proc. 25th Int. Conf. Pattern Recognit. (ICPR), [] C. Lassner and M. Zollhofer, ‘‘Pulsar: Efﬁcient sphere-based neural rendering,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] C. Wu, Z. Xiu, Y. Shi, O. Kalinli, C. Fuegen, T. Koehler, and Q. He, ‘‘Transformer-based acoustic modeling for streaming speech synthesis,’’ in Proc. Interspeech, , pp. –. [] A. Richard, D. Markovic, I. D. Gebru, S. Krenn, G. A. Butler, F. Torre, and Y. Sheikh, ‘‘Neural synthesis of binaural speech from mono audio,’’ in Proc. Int. Conf. Learn. Represent., , pp. –. [] J. D. Won, ‘‘Control strategies for physically simulated characters performing two-player competitive sports,’’ ACM Trans. Graph., vol. , [] Y. Ye, S. Tulsiani, and A. Gupta, ‘‘Shelf-supervised mesh prediction in the wild,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), [] W. Yuan, Z. Lv, T. Schmidt, and S. Lovegrove, ‘‘STaR: Self-supervised tracking and reconstruction of rigid objects in motion with neural rendering,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] E. Ng, S. Ginosar, T. Darrell, and H. Joo, ‘‘Body2Hands: Learning to infer 3D hands from conversational gesture body dynamics,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , [] M. Eisenberger, D. Novotny, G. Kerchenbaum, P. Labatut, N. Neverova, ‘‘NeuroMorph: Unsupervised shape interpolation and correspondence in one go,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] B.-C. Chen, Z. Wu, L. S. Davis, and S.-N. Lim, ‘‘Efﬁcient object embedding for spliced image retrieval,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] D. Kiela, M. Bartolo, Y. Nie, D. Kaushik, A. Geiger, Z. Wu, B. Vidgen, G. Prasad, A. Singh, P. Ringshia, Z. Ma, T. Thrush, S. Riedel, Z. Waseem, P. Stenetorp, R. Jia, M. Bansal, C. Potts, and A. Williams, ‘‘Dynabench: Rethinking benchmarking in NLP,’’ , arXiv:.. [] T. Blevins, M. Joshi, and L. Zettlemoyer, low-shot word sense disambiguation with the dictionary,’’ , [] N. De Cao, G. Izacard, S. Riedel, and F. Petroni, ‘‘Autoregressive entity retrieval,’’ , arXiv:.. S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges [] O. Gafni, O. Ashual, and L. Wolf, ‘‘Single-shot freestyle dance reenactment,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] C. Gong, D. Wang, M. Li, V. Chandra, and Q. Liu, ‘‘KeepAug- ment: A simple information-preserving data augmentation approach,’’ Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. , [] B. Li, F. Wu, S.-N. Lim, S. Belongie, and K. Q. Weinberger, ‘‘On feature normalization and data augmentation,’’ Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] K. J Liang, W. Hao, D. Shen, Y. Zhou, W. Chen, C. Chen, and L. Carin, ‘‘MixKD: Towards efﬁcient distillation of large-scale language models,’’ [] M. Jia, Z. Wu, A. Reiter, C. Cardie, S. Belongie, and S.-N. Lim, ‘‘Intentonomy: A dataset and study towards human intent understand- ing,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), [] Q. Huang, H. He, A. Singh, S.-N. Lim, and A. R. Benson, ‘‘Combining label propagation and simple models out-performs graph neural net- works,’’ , arXiv:.. [] J. Ahlgren, K. Bojarczuk, S. Drossopoulou, I. Dvortsova, J. George, N. Gucevska, M. Harman, M. Lomeli, S. M. Lucas, E. Meijer, and S. Omohundro, ‘‘Facebook’s cyber–cyber and cyber–physical digital twins,’’ in Evaluation and Assessment in Software Engineering. Trond- heim, Norway: Norwegian Univ. Science and Technology (NTNU), , [] BlenderBot . GitHub. Accessed: Nov. , . [Online]. Available: https://github.com/facebookresearch/ParlAI/tree/master/projects/ [] G. Dagan, D. Hupkes, and E. Bruni, ‘‘Co-evolution of language and agents in referential games,’’ , arXiv:.. [] D. Le, G. Keren, J. Chan, J. Mahadeokar, C. Fuegen, and M. L. Seltzer, ‘‘Deep shallow fusion for RNN-T personalization,’’ in Proc. IEEE Spoken Lang. Technol. Workshop (SLT), Jan. , pp. –. [] L. Weber, J. Jumelet, E. Bruni, and D. Hupkes, ‘‘Language modelling as a multi-task problem,’’ , arXiv:.. [] P. Lewis, B. Oğuz, R. Rinott, S. Riedel, and H. Schwenk, ‘‘MLQA: [] W. Xiong, X. Lorraine Li, S. Iyer, J. Du, P. Lewis, W. Y. Wang, Y. Mehdad, W.-T. Yih, S. Riedel, D. Kiela, and B. Oğuz, ‘‘Answering complex open-domain questions with multi-hop dense retrieval,’’ , [] S. Min, J. Boyd-Graber, C. Alberti, D. Chen, E. Choi, M. Collins, K. Guu, H. Hajishirzi, K. Lee, J. Palomaki, and C. Raffel, ‘‘NeurIPS  EfﬁcientQA competition: Systems, analyses and lessons learned,’’ , [] H. Schwenk, V. Chaudhary, S. Sun, H. Gong, and F. Guzmán, ‘‘WikiMatrix: Mining 135M parallel sentences in  language pairs from Wikipedia,’’ , arXiv:.. [] Y. Tang, J. Pino, X. Li, C. Wang, and D. Genzel, ‘‘Improving speech translation by understanding and learning from the auxiliary text translation task,’’ in Proc. 59th Annu. Meeting Assoc. Comput. Linguistics, 11th Int. Joint Conf. Natural Lang. Process., , [] Y.-L. Tuan, A. El-Kishky, A. Renduchintala, V. Chaudhary, F. Guzmán, and L. Specia, ‘‘Quality estimation without human-labeled data,’’ , [] M. Patrick, P.-Y. Huang, Y. Asano, F. Metze, A. Hauptmann, J. Henriques, and A. Vedaldi, ‘‘Support-set bottlenecks for video-text representation learning,’’ , arXiv:.. [] Z. Meng, L. Yu, N. Zhang, T. Berg, B. Damavandi, V. Singh, and A. Bearman, ‘‘Connecting what to say with where to look by modeling human attention traces,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] P. Morgado, I. Misra, and N. Vasconcelos, ‘‘Robust audio-visual instance discrimination,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] P. Morgado, N. Vasconcelos, and I. Misra, ‘‘Audio-visual discrimination with cross-modal agreement,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] A. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. Turner, N. Maestre, M. Mukadam, D. S. Chaplot, O. Maksymets, and A. Gokaslan, ‘‘Habitat .: Training home assistants to rearrange their habitat,’’ , arXiv:.. [] C. Chen, S. Majumder, Z. Al-Halah, R. Gao, S. Kumar Ramakrishnan, and K. Grauman, ‘‘Learning to set waypoints for audio-visual naviga- tion,’’ , arXiv:.. [] C. Chen, Z. Al-Halah, and K. Grauman, ‘‘Semantic audio-visual navigation,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] J. Ye, D. Batra, A. Das, and E. Wijmans, ‘‘Auxiliary tasks and exploration enable ObjectNav,’’ , arXiv:.. [] T. Bagautdinov, C. Wu, T. Simon, F. Prada, T. Shiratori, S.-E. Wei, W. Xu, Y. Sheikh, and J. Saragih, ‘‘Driving-signal aware full-body avatars,’’ , arXiv:.. [] S. Lombardi, T. Simon, G. Schwartz, M. Zollhoefer, Y. Sheikh, and J. Saragih, ‘‘Mixture of volumetric primitives for efﬁcient neural rendering,’’ , arXiv:.. [] T. Sun, G. Nam, C. Aliaga, C. Hery, and R. Ramamoorthi, ‘‘Human hair inverse rendering using multi-view photometric data,’’ in Proc. Eurograph. Symp. Rendering (EGSR), , pp. –. [] D. Xiang, F. Prada, T. Bagautdinov, W. Xu, Y. Dong, H. Wen, J. Hodgins, and C. Wu, ‘‘Modeling clothing as a separate layer for an animatable human avatar,’’ , arXiv:.. [] B. Chaudhuri, N. Saraﬁanos, L. Shapiro, and T. Tung, ‘‘Semi-supervised synthesis of high-resolution editable textures for 3D humans,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , [] S. Bi, S. Lombardi, S. Saito, T. Simon, S. E. Wei, K. McPhail, R. Ramamoorthi, Y. Sheikh, and J. Saragih, ‘‘Deep relightable appearance models for animatable faces,’’ ACM Trans. Graph., vol. , pp. –, [] S. Ma, T. Simon, J. Saragih, D. Wang, Y. Li, F. D. La Torre, and Y. Sheikh, ‘‘Pixel codec avatars,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] A. Shukla, S. Petridis, and M. Pantic, ‘‘Does visual self-supervision improve learning of speech representations for emotion recognition,’’ IEEE Trans. Affect. Comput., early access, Feb. , , doi: [] M. Jiang, E. Grefenstette, and T. Rocktäschel, ‘‘Prioritized level replay,’’ in Proc. Int. Conf. Mach. Learn., , pp. –. [] N. Modhe, P. Chattopadhyay, M. Sharma, A. Das, D. Parikh, D. Batra, and R. Vedantam, ‘‘IR-VIC: Unsupervised discovery of sub-goals for transfer in RL,’’ , arXiv:.. [] B. Cui, Y. Chow, and M. Ghavamzadeh, ‘‘Control-aware representations for model-based reinforcement learning,’’ , arXiv:.. [] B. Amos, S. Stanton, D. Yarats, and A. G. Wilson, ‘‘On the model- based stochastic value gradient for continuous reinforcement learning,’’ in Learning for Dynamics and Control. Cambridge, U.K.: Proceedings of Machine Learning Research, . [] S. Sukhbaatar, D. Ju, S. Poff, S. Roller, A. Szlam, J. Weston, and A. Fan, ‘‘Not all memories are created equal: Learning to forget by expiring,’’ [] M. Abdelsalam, M. Faramarzi, S. Sodhani, IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , [] V. K. Verma, K. J. Liang, N. Mehta, P. Rai, and L. Carin, ‘‘Efﬁcient feature transformations for discriminative and generative continual learning,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), [] N. Mehta, K. Liang, V. K. Verma, and L. Carin, ‘‘Continual learning using a Bayesian nonparametric dictionary of weight factors,’’ in Proc. Int. Conf. Artif. Intell. Statist., , pp. –. [] S. Ebrahimi, S. Petryk, A. Gokul, W. Gan, J. E. Gonzalez, M. Rohrbach, and T. Darrell, ‘‘Remembering for the right reasons: Explanations reduce catastrophic forgetting,’’ , arXiv:.. [] S. Sodhani, A. Zhang, and J. Pineau, ‘‘Multi-task reinforcement learning with context-based representations,’’ , arXiv:.. [] C. Fu, H. Huang, X. Chen, Y. Tian, and J. Zhao, ‘‘Learn-to-share: A hardware-friendly transfer learning framework exploiting computation and parameter sharing,’’ in Proc. Int. Conf. Mach. Learn., , [] A. Zhang, S. Sodhani, K. Khetarpal, and J. Pineau, robust state abstractions for hidden-parameter block MDPs,’’ , [] P. Dollar, M. Singh, and R. Girshick, ‘‘Fast and accurate model scaling,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), S.-M. Park, Y.-G. Kim: Metaverse: Taxonomy, Components, Applications, and Open Challenges [] A. Ruiz and J. Verbeek, ‘‘Anytime inference with distilled hierarchical neural ensembles,’’ in Proc. AAAI Conf. Artif. Intell., vol. , no. , , [] B. Acun, M. Murphy, X. Wang, J. Nie, C.-J. Wu, and K. Hazelwood, ‘‘Understanding training efﬁciency of deep learning recommendation models at scale,’’ in Proc. IEEE Int. Symp. High-Performance Comput. Archit. (HPCA), Feb. , pp. –. [] H. D. Dixit, S. Pendharkar, M. Beadon, C. Mason, T. Chakravarthy, B. Muthiah, and S. Sankar, ‘‘Silent data corruptions at scale,’’ , [] Y. Zhao, L. Wang, Y. Tian, R. Fonseca, and T. Guo, ‘‘Few-shot neural architecture search,’’ in Proc. Int. Conf. Mach. Learn., , [] D. Wang, M. Li, C. Gong, and V. Chandra, ‘‘AttentiveNAS: Improving neural architecture search via attentive sampling,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. , pp. –. [] B. Zhang, R. Rajan, L. Pineda, N. Lambert, A. Biedenkapp, K. Chua, F. Hutter, and R. Calandra, ‘‘On the importance of hyperparameter optimization for model-based reinforcement learning,’’ in Proc. Int. Conf. Artif. Intell. Statist., , pp. –. [] A. Zhang, R. McAllister, R. Calandra, Y. Gal, and S. Levine, ‘‘Learning invariant representations for reinforcement learning without reconstruc- tion,’’ , arXiv:.. [] M. Panchenko, R. Auler, L. Sakka, and G. Ottoni, ‘‘Lightning BOLT: Powerful, fast, and scalable binary optimization,’’ in Proc. 30th ACM SIGPLAN Int. Conf. Compiler Construct., Mar. , pp. –. [] T. Li, A. Beirami, M. Sanjabi, and V. Smith, ‘‘Tilted empirical risk minimization,’’ , arXiv:.. [] T. Li, S. Hu, A. Beirami, and V. Smith, ‘‘Ditto: Fair and robust federated learning through personalization,’’ in Proc. Int. Conf. Mach. Learn., , [] K. Maeng, S. Bharuka, I. Gao, M. Jeffrey, V. Saraph, B.-Y. Su, C. Trippel, J. Yang, M. Rabbat, B. Lucia, and C. J. Wu, ‘‘Understanding and improving failure tolerant training for deep learning recommendation with partial recovery,’’ Proc. Mach. Learn. Syst., vol. , pp. –, [] F. Lin, B. Bolla, E. Pinkham, N. Kodner, D. Moore, A. Desai, and S. Sankar, ‘‘Near-realtime server reboot monitoring and root cause analysis in a large-scale system,’’ in Proc. 51st Annu. IEEE/IFIP Int. Conf. Dependable Syst. Netw.-Supplemental (DSN-S), , pp. –, doi: . [] Y. Xia, Y. Zhang, Z. Zhong, G. Yan, C. Lim, S. S. Ahuja, S. Bali, A. Nikolaidis, K. Ghobadi, and M. Ghobadi, ‘‘A social network under social distancing: Risk-driven backbone management during COVID- and beyond,’’ in Proc. NSDI, , pp. –. [] E. J. Oughton, W. Lehr, K. Katsaros, I. Selinis, D. Bubley, and J. Kusuma, ‘‘Revisiting wireless internet connectivity: 5G vs Wi-Fi ,’’ Telecommun. Policy, vol. , no. , Jun. , Art. no. . [] J. Lin, Y. Wang, K. Kalgaonkar, G. Keren, D. Zhang, and C. Fuegen, ‘‘A time-domain convolutional recurrent network for packet loss concealment,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Jun. , pp. –. [] B. Reagen, W.-S. Choi, Y. Ko, V. T. Lee, H.-H.-S. Lee, G.-Y. Wei, and D. Brooks, ‘‘Cheetah: Optimizing and accelerating homomorphic encryp- tion for private inference,’’ in Proc. IEEE Int. Symp. High-Performance Comput. Archit. (HPCA), Feb. , pp. –. [] M. Cowan, D. Dangwal, A. Alaghi, C. Trippel, V. T. Lee, and B. Reagen, ‘‘Porcupine: A synthesizing compiler for vectorized homo- morphic encryption,’’ in Proc. 42nd ACM SIGPLAN Int. Conf. Program. Lang. Design Implement., Jun. , pp. –. [] S. Belkhale, R. Li, G. Kahn, R. McAllister, R. Calandra, and S. Levine, ‘‘Model-based meta-reinforcement learning for ﬂight with suspended payloads,’’ IEEE Robot. Autom. Lett., vol. , no. , pp. –, [] J. C. Shih, F. Meier, and A. Rai, ‘‘A framework for online updates to safe sets for uncertain dynamics,’’ in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), Oct. , pp. –. [] E. Giraudy, P. Maas, S. Iyer, Z. Almquist, J. W. Schneider, and A. Dow, ‘‘Measuring long-term displacement using Facebook data,’’ IDMC Global Rep. Internal Displacement (GRID), Geneva, Switzerland, [] J. Gray, A. Lerer, A. Bakhtin, and N. Brown, ‘‘Human-level performance in no-press diplomacy via equilibrium search,’’ , arXiv:.. [] V. Ha-Thuc, M. Wood, Y. Liu, and J. Sundaresan, ‘‘From producer success to retention: A new role of search and recommendation systems on marketplaces,’’ in Proc. 44th Int. ACM SIGIR Conf. Res. Develop. Inf. Retr., Jul. , pp. –. [] S. Blackshear, K. Chalkias, P. Chatzigiannis, R. Faizullabhoy, I. Khaburzaniya, E. K. Kogias, J. Lind, D. Wong, and T. Zakian, ‘‘Reactive key-loss protection in blockchains,’’ IACR Cryptol. ePrint Arch., vol. , p. , Mar. . [] V. Conitzer, C. Kroer, E. Sodomka, and N. E. Stier-Moses, ‘‘Multiplica- tive pacing equilibria in auction markets,’’ , arXiv:.. [] J. Onaolapo, N. Leontiadis, D. Magka, and G. Stringhini, ‘‘SocialHEIST- ing: Understanding stolen Facebook accounts,’’ in Proc. 30th USENIX Secur. Symp. (USENIX Security), , pp. –. [] M. Bailey, P. Farrell, T. Kuchler, and J. Stroebel, ‘‘Social connectedness in urban areas,’’ J. Urban Econ., vol. , Jul. , Art. no. . [] M. Luria and N. Foulds, ‘‘Hashtag-forget: Using social media ephemer- ality to support evolving identities,’’ in Proc. Extended Abstr. CHI Conf. Hum. Factors Comput. Syst., May , pp. –. [] V. Do, J. Atif, J. Lang, and N. Usunier, ‘‘Online selection of diverse committees,’’ , arXiv:.. ‘‘Diverse and inclusive representation in online advertising: An exploration of people’s expectations,’’ Meta, CA, USA, Tech. Rep., Mar. . Accessed: Nov. , . [Online]. Available: https://research.facebook. com/publications/diverse-and-inclusive-representation-in-online- advertising-an-exploration-of-the-current-landscape-and-peoples- [] V. Avadhanula, R. C. Baldeschi, S. Leonardi, K. A. Sankararaman, and O. Schrijvers, ‘‘Stochastic bandits for multi-platform budget optimization in online advertising,’’ in Proc. Web Conf., Apr. , pp. –. [] D. Sinha, K. A. Sankararaman, A. Kazerouni, and V. Avadhanula, ‘‘Multi- armed bandits with cost subsidy,’’ in Proc. Int. Conf. Artif. Intell. Statist., [] J. Truong, S. Chernova, and D. Batra, ‘‘Bi-directional domain adaptation for Sim2Real transfer of embodied navigation agents,’’ IEEE Robot. Autom. Lett., vol. , no. , pp. –, Apr. . [] Detectron2 GitHub. Accessed: Nov. , . [Online]. Available: https://github.com/facebookresearch/detectron2 SANG-MIN PARK received the Ph.D. degree in computer engineering from the Department of Computer Science and Engineering, Korea University, in . His current research interests science, sentiment analysis, causal multi-modal analysis, personalized service, gener- ative model, and reinforcement learning. YOUNG-GAB KIM (Member, IEEE) received the B.S. degree in biotechnology and genetic engineer- ing (minor in computer science and engineering) and the M.S. and Ph.D. degrees in computer science and engineering from Korea University, Seoul, South Korea, in , , and , respectively. He was an Assistant Professor with the School of Information Technology, Catholic University of Daegu. He is currently an Associate Professor with the Department of Computer and Information Security and Convergence Engineering for Intelligent Drone, Sejong University. As a Korean ISO/IECJTC1 Member, he has contributed to the development of data exchange standards. He has published more than  research articles in the ﬁelds of computer science and infor- mation security. His current research interests include big data security, network security, home networks, security risk analysis, and security